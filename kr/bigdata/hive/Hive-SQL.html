<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hive SQL 执行计划|数据倾斜|性能优化 | Keep Running</title>
    <meta name="generator" content="VuePress 1.9.7">
    <link rel="icon" href="/favicon.png">
    <script charset="utf-8" async="async" src="/js/jquery.min.js"></script>
    <script charset="utf-8" async="async" src="/js/global.js"></script>
    <script charset="utf-8" async="async" src="/js/fingerprint2.min.js"></script>
    <meta name="description" content="">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">
    <meta name="apple-mobile-web-app-capable" content="yes">
    
    <link rel="preload" href="/assets/css/0.styles.42b858ce.css" as="style"><link rel="preload" href="/assets/js/app.cfd60917.js" as="script"><link rel="preload" href="/assets/js/4.8234885e.js" as="script"><link rel="preload" href="/assets/js/3.1429892b.js" as="script"><link rel="preload" href="/assets/js/26.fc7addb7.js" as="script"><link rel="prefetch" href="/assets/js/10.0f0de338.js"><link rel="prefetch" href="/assets/js/11.0c47d372.js"><link rel="prefetch" href="/assets/js/12.b45ce082.js"><link rel="prefetch" href="/assets/js/13.f2032214.js"><link rel="prefetch" href="/assets/js/14.0a8927b2.js"><link rel="prefetch" href="/assets/js/15.9ab1d206.js"><link rel="prefetch" href="/assets/js/16.f03f2e40.js"><link rel="prefetch" href="/assets/js/17.c6bbe2be.js"><link rel="prefetch" href="/assets/js/18.e08bb862.js"><link rel="prefetch" href="/assets/js/19.c00af7cb.js"><link rel="prefetch" href="/assets/js/20.35396483.js"><link rel="prefetch" href="/assets/js/21.fe05d239.js"><link rel="prefetch" href="/assets/js/22.80eaa289.js"><link rel="prefetch" href="/assets/js/23.bce5e854.js"><link rel="prefetch" href="/assets/js/24.fa104861.js"><link rel="prefetch" href="/assets/js/25.2c725be9.js"><link rel="prefetch" href="/assets/js/27.fc831a66.js"><link rel="prefetch" href="/assets/js/28.3784f580.js"><link rel="prefetch" href="/assets/js/29.7e78aa5c.js"><link rel="prefetch" href="/assets/js/30.b074fddd.js"><link rel="prefetch" href="/assets/js/31.059ebf27.js"><link rel="prefetch" href="/assets/js/32.12570018.js"><link rel="prefetch" href="/assets/js/33.d0dc971f.js"><link rel="prefetch" href="/assets/js/34.c9b46696.js"><link rel="prefetch" href="/assets/js/35.77a81c6a.js"><link rel="prefetch" href="/assets/js/36.6048e328.js"><link rel="prefetch" href="/assets/js/37.b9f285de.js"><link rel="prefetch" href="/assets/js/38.ac5a2abf.js"><link rel="prefetch" href="/assets/js/39.97ca52cb.js"><link rel="prefetch" href="/assets/js/40.1689d33f.js"><link rel="prefetch" href="/assets/js/41.4d2d8182.js"><link rel="prefetch" href="/assets/js/42.8647a8ff.js"><link rel="prefetch" href="/assets/js/43.f867a280.js"><link rel="prefetch" href="/assets/js/44.a51272ee.js"><link rel="prefetch" href="/assets/js/45.c31e6474.js"><link rel="prefetch" href="/assets/js/46.fe85ae54.js"><link rel="prefetch" href="/assets/js/47.dc8a97d4.js"><link rel="prefetch" href="/assets/js/48.c97f7726.js"><link rel="prefetch" href="/assets/js/49.ac219d14.js"><link rel="prefetch" href="/assets/js/5.963fada3.js"><link rel="prefetch" href="/assets/js/50.926ecfa5.js"><link rel="prefetch" href="/assets/js/51.e776b5b1.js"><link rel="prefetch" href="/assets/js/52.4938314c.js"><link rel="prefetch" href="/assets/js/53.6b4d7266.js"><link rel="prefetch" href="/assets/js/54.bfaf622b.js"><link rel="prefetch" href="/assets/js/55.c6d160b9.js"><link rel="prefetch" href="/assets/js/56.31f6843c.js"><link rel="prefetch" href="/assets/js/57.55cbf7b8.js"><link rel="prefetch" href="/assets/js/58.8174871d.js"><link rel="prefetch" href="/assets/js/59.3eee2bde.js"><link rel="prefetch" href="/assets/js/6.b2e36a75.js"><link rel="prefetch" href="/assets/js/60.233a32ed.js"><link rel="prefetch" href="/assets/js/61.defb83ac.js"><link rel="prefetch" href="/assets/js/62.442070cd.js"><link rel="prefetch" href="/assets/js/63.cc121456.js"><link rel="prefetch" href="/assets/js/64.c8f83f5d.js"><link rel="prefetch" href="/assets/js/65.b9bc8c1c.js"><link rel="prefetch" href="/assets/js/7.fb6689bf.js"><link rel="prefetch" href="/assets/js/8.e390cc6b.js"><link rel="prefetch" href="/assets/js/9.c03c9dfe.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.db8a86c9.js">
    <link rel="stylesheet" href="/assets/css/0.styles.42b858ce.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Keep Running</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/kr/java/jvm/自动内存管理.html" class="nav-link">
  Java
</a></div><div class="nav-item"><a href="/kr/database/database.html" class="nav-link">
  数据库
</a></div><div class="nav-item"><a href="/kr/spring/MVC-IOC-AOP.html" class="nav-link">
  Spring
</a></div><div class="nav-item"><a href="/kr/middleware/redis/redis-base.html" class="nav-link">
  Redis/Zookeeper/Kafka
</a></div><div class="nav-item"><a href="/kr/bigdata/hadoop/Hadoop-分布式集群容器部署.html" class="nav-link">
  大数据
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/kr/java/jvm/自动内存管理.html" class="nav-link">
  Java
</a></div><div class="nav-item"><a href="/kr/database/database.html" class="nav-link">
  数据库
</a></div><div class="nav-item"><a href="/kr/spring/MVC-IOC-AOP.html" class="nav-link">
  Spring
</a></div><div class="nav-item"><a href="/kr/middleware/redis/redis-base.html" class="nav-link">
  Redis/Zookeeper/Kafka
</a></div><div class="nav-item"><a href="/kr/bigdata/hadoop/Hadoop-分布式集群容器部署.html" class="nav-link">
  大数据
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Hadoop</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/kr/bigdata/hadoop/Hadoop-分布式集群容器部署.html" class="sidebar-link">Hadoop概述及分布式集群容器化部署</a></li><li><a href="/kr/bigdata/hadoop/分布式文件系统-HDFS-体系架构及原理.html" class="sidebar-link">分布式文件系统 HDFS 体系架构及原理</a></li><li><a href="/kr/bigdata/hadoop/分布式并行计算-MapReduce-概述及原理.html" class="sidebar-link">分布式并行计算 MapReduce 概述及原理</a></li><li><a href="/kr/bigdata/hadoop/集群资源管理与调度平台-YARN-概述及原理.html" class="sidebar-link">集群资源管理与调度平台 YARN 概述及原理</a></li><li><a href="/kr/bigdata/hadoop/MapReduce-实例-WordCount.html" class="sidebar-link">MapReduce 经典案例：WordCount</a></li><li><a href="/kr/bigdata/hadoop/HDFS-3-X-纠删码.html" class="sidebar-link">HDFS 3.X 纠删码</a></li><li><a href="/kr/bigdata/hadoop/大数据生态圈与离线实时数据平台实践.html" class="sidebar-link">大数据生态圈与离线实时数据平台实践</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Hive</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/kr/bigdata/hive/分布式数据仓库-Hive.html" class="sidebar-link">分布式数据仓库 Hive</a></li><li><a href="/kr/bigdata/hive/Hive-SQL.html" aria-current="page" class="active sidebar-link">Hive SQL 执行计划|数据倾斜|性能优化</a></li><li><a href="/kr/bigdata/hive/HiveSQL-编译过程.html" class="sidebar-link">Hive 工作原理：Hive SQL 编译过程</a></li><li><a href="/kr/bigdata/hive/Hive-function.html" class="sidebar-link">Hive Function</a></li><li><a href="/kr/bigdata/hive/Hive-SQL-Examples.html" class="sidebar-link">Hive SQL Examples</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Spark</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/kr/bigdata/spark/Spark-概述与集群部署模式.html" class="sidebar-link">Spark 概述与集群部署模式</a></li><li><a href="/kr/bigdata/spark/Spark-RDD-弹性分布式数据集.html" class="sidebar-link">Spark RDD 弹性分布式数据集</a></li><li><a href="/kr/bigdata/spark/Spark-运行架构.html" class="sidebar-link">Spark 运行架构及作业提交流程</a></li><li><a href="/kr/bigdata/spark/Spark-Streaming.html" class="sidebar-link">Spark Streaming</a></li><li><a href="/kr/bigdata/spark/Spark-sql.html" class="sidebar-link">Spark SQL</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Other</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/kr/bigdata/Canal-MySQL-binlog-增量订阅与消费组件.html" class="sidebar-link">Canal：MySQL Binlog 增量订阅与消费组件</a></li><li><a href="/kr/bigdata/Sqoop-数据迁移工具.html" class="sidebar-link">Sqoop 数据迁移工具</a></li><li><a href="/kr/bigdata/Flume-基本架构及应用场景.html" class="sidebar-link">Flume 基本架构及应用场景</a></li><li><a href="/kr/bigdata/Azkaban-编译镜像及基本使用.html" class="sidebar-link">Azkaban 编译镜像及基本使用</a></li><li><a href="/kr/bigdata/Maxwell.html" class="sidebar-link">Maxwell</a></li><li><a href="/kr/bigdata/Elaticsearch.html" class="sidebar-link">Elaticsearch</a></li><li><a href="/kr/bigdata/大数据可视化交互平台-Hue.html" class="sidebar-link">大数据可视化交互平台 Hue</a></li></ul></section></li></ul> </aside> <div><main class="page"> <div class="theme-default-content content__default"><h1 id="hive-sql-执行计划-数据倾斜-性能优化"><a href="#hive-sql-执行计划-数据倾斜-性能优化" class="header-anchor">#</a> Hive SQL 执行计划|数据倾斜|性能优化</h1> <h2 id="执行计划"><a href="#执行计划" class="header-anchor">#</a> 执行计划</h2> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">EXPLAIN</span> <span class="token punctuation">[</span><span class="token keyword">EXTENDED</span> <span class="token operator">|</span> DEPENDENCY <span class="token operator">|</span> <span class="token keyword">AUTHORIZATION</span><span class="token punctuation">]</span> query
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>通过执行计划能了解 SQL 在转换成相应计算引擎的执行逻辑，掌握了执行逻辑也就能更好地把握程序出现的瓶颈点，从而能够实现更有针对性的优化。Hive 集成了 Apache Calcite，使得 Hive 能够基于成本代价来生成执行计划，这种方式能够结合 Hive 元数据信息和 Hive  运行过程收集的各类统计信息推测出一个更为合理的执行计划，Hive  目前所提供的执行计划都是预估。</p> <p>Hive 执行计划提供的信息：</p> <ul><li>查看执行计划的基本信息（explain）</li> <li>查看执行计划的扩展信息（explain extended）</li> <li>查看 SQL 数据输入依赖的信息（explain dependency）</li> <li>查看 SQL 操作相关权限的信息（explain authorization）</li> <li>查看 SQL 的向量化描述信息（explain vectorization）</li></ul> <h3 id="explaint"><a href="#explaint" class="header-anchor">#</a> explaint</h3> <p>explaint 执行计划包含两部分：</p> <ul><li>作业的依赖关系图，即 STAGE DEPENDENCIES</li> <li>每个作业的详细信息，即 STAGE PLANS</li></ul> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># HQL 语句被转换为由多个 stage(阶段) 组成的序列（有向无环图 DAG），</span>
<span class="token comment"># 这些 stage 包括：MR stage、元数据存储的 stage、文件系统操作的 stage</span>
<span class="token comment"># 有 MR 作业</span>
<span class="token number">0</span>: jdbc:hive2://localhost:1000<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span> explain <span class="token keyword">select</span> * from score cluster by s_id<span class="token punctuation">;</span>
STAGE DEPENDENCIES:                     <span class="token comment"># 各个 stage 间的依赖关系</span>
  Stage-1 is a root stage               <span class="token comment"># root stage，执行的起始 stage</span>
  Stage-0 depends on stages: Stage-1    <span class="token comment"># 依赖于 stage-1</span>

STAGE PLANS:                            <span class="token comment"># 各个 stage 的执行计划</span>
  Stage: Stage-1                        <span class="token comment"># stage-1 的执行计划</span>
    Map Reduce                          <span class="token comment"># MapReduce 作业的执行计划</span>
      Map Operator Tree:                <span class="token comment"># Map 端的执行计划树</span>
          TableScan                     <span class="token comment"># 表扫描操作</span>
            alias: score                <span class="token comment"># 表名称/别名</span>
            Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">1620</span> Basic stats: COMPLETE Column stats: NONE <span class="token comment"># 表统计信息</span>
            Select Operator             <span class="token comment"># 选取操作</span>
              expressions: s_id <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, c_id <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, s_score <span class="token punctuation">(</span>type: int<span class="token punctuation">)</span> <span class="token comment"># 选取需要查询的表字段名/类型</span>
              outputColumnNames: _col0, _col1, _col2        <span class="token comment"># 输出的列名</span>
              Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">1620</span> Basic stats: COMPLETE Column stats: NONE <span class="token comment"># 表统计信息</span>
              Reduce Output Operator    <span class="token comment"># 输出到 Reduce 的操作</span>
                key expressions: _col0 <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span> <span class="token comment"># Map key</span>
                <span class="token function">sort</span> order: +           <span class="token comment"># 排序</span>
                Map-reduce partition columns: _col0 <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>
                Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">1620</span> Basic stats: COMPLETE Column stats: NONE
                value expressions: _col1 <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, _col2 <span class="token punctuation">(</span>type: int<span class="token punctuation">)</span> <span class="token comment"># Map value</span>
      Execution mode: vectorized
      Reduce Operator Tree:             <span class="token comment"># Reduce 端的执行计划树</span>
        Select Operator                 <span class="token comment"># 选取操作</span>
          expressions: KEY.reducesinkkey0 <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, VALUE._col0 <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, VALUE._col1 <span class="token punctuation">(</span>type: int<span class="token punctuation">)</span>
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">1620</span> Basic stats: COMPLETE Column stats: NONE
          File Output Operator          <span class="token comment"># 文件输出操作</span>
            compressed: <span class="token boolean">false</span>           <span class="token comment"># 是否压缩</span>
            Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">1620</span> Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator                      <span class="token comment"># 客户端取数据操作</span>
      limit: -1
      Processor Tree:
        ListSink
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br></div></div><h4 id="常见-operator-操作"><a href="#常见-operator-操作" class="header-anchor">#</a> 常见 operator 操作</h4> <p>Map/Reduce 的执行计划中包含常见的 operator 操作：</p> <ul><li>TableScan 表扫描操作
<ul><li>alias：表名称/别名</li> <li>Statistics：表统计信息</li></ul></li> <li>Select Operator 选取操作
<ul><li>expressions：需要查询的表字段名/类型</li> <li>outputColumnNames：输出的列名称</li> <li>Statistics：表统计信息</li></ul></li> <li>Group By Operator 分组聚合操作
<ul><li>aggregations：显示聚合函数信息</li> <li>mode：聚合模式，值有 hash：随机聚合(hash partition)；partial：局部聚合；final：最终聚合</li> <li>keys：分组的字段</li> <li>outputColumnNames：聚合之后输出列名</li> <li>Statistics：表统计信息，包含分组聚合之后的数据条数，数据大小等</li></ul></li> <li>Reduce Output Operator 输出到 Reduce 操作
<ul><li>sort order：值为空不排序；+ 正序排序；- 倒序排序；+- 排序的列为两列，第一列为正序，第二列为倒序</li></ul></li> <li>Filter Operator 过滤操作
<ul><li>predicate：过滤条件</li></ul></li> <li>Map Join Operator join 操作
<ul><li>condition map：join 方式</li> <li>keys：join 的条件字段</li> <li>outputColumnNames：join 完成之后输出的字段</li> <li>Statistics：join 完成之后生成的数据条数，大小等</li></ul></li> <li>File Output Operator 文件输出操作
<ul><li>compressed：是否压缩</li> <li>table：表的信息，包含输入输出文件格式化方式，序列化方式等</li></ul></li> <li>Fetch Operator 客户端获取数据操作
<ul><li>limit：值为 -1 表示不限制条数，其他值为限制的条数</li></ul></li></ul> <h4 id="join-会过滤-null-值"><a href="#join-会过滤-null-值" class="header-anchor">#</a> Join 会过滤 null 值？</h4> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">explain</span> <span class="token keyword">select</span> s<span class="token punctuation">.</span>s_id<span class="token punctuation">,</span> s<span class="token punctuation">.</span>s_name<span class="token punctuation">,</span> o<span class="token punctuation">.</span>c_id<span class="token punctuation">,</span> o<span class="token punctuation">.</span>s_score <span class="token keyword">from</span> student s <span class="token keyword">join</span> score o <span class="token keyword">on</span> s<span class="token punctuation">.</span>s_id <span class="token operator">=</span> o<span class="token punctuation">.</span>s_id<span class="token punctuation">;</span>
<span class="token comment">-- 通过执行计划的 predicate: s_id is not null (type: boolean) 可以看出在 join 两个表时会过滤 null 值</span>
<span class="token comment">-- 但 left join 和 full join 不会过滤 null 值</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><img src="/images/kr/bigdata/hive/%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-Join-null%E5%A4%84%E7%90%86.png" alt=""></p> <h4 id="group-by-会进行排序"><a href="#group-by-会进行排序" class="header-anchor">#</a> Group by 会进行排序？</h4> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">explain</span> <span class="token keyword">select</span> s_id<span class="token punctuation">,</span> <span class="token function">max</span><span class="token punctuation">(</span>s_score<span class="token punctuation">)</span> <span class="token keyword">from</span> score <span class="token keyword">group</span> <span class="token keyword">by</span> s_id<span class="token punctuation">;</span>
<span class="token comment">-- keys: s_id (type: string)： 按照 s_id 分组；sort order: + 正序排序 </span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><img src="/images/kr/bigdata/hive/%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-group-by-%E6%8E%92%E5%BA%8F.png" alt=""></p> <h3 id="explain-dependency"><a href="#explain-dependency" class="header-anchor">#</a> explain dependency</h3> <p>explain dependency 用于描述 SQL 数据来源，输出一个 json 格式数据，常用于分析 SQL 数据过滤情况，两部分：</p> <ul><li>input_partitions：SQL 数据来源表分区，存储的是分区名，若整段 SQL 所有表都是非分区表，则为空</li> <li>input_tables：SQL 数据来源表，存储的是 Hive 表名</li></ul> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">explain</span> dependency <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ods_order_info i <span class="token keyword">left</span> <span class="token keyword">join</span> ods_order_detail d <span class="token keyword">on</span> i<span class="token punctuation">.</span>id <span class="token operator">=</span> d<span class="token punctuation">.</span>order_id <span class="token operator">and</span> i<span class="token punctuation">.</span>dt <span class="token operator">&gt;</span> <span class="token string">'2021-09-26'</span><span class="token punctuation">;</span>
<span class="token keyword">explain</span> dependency <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ods_order_info i <span class="token keyword">left</span> <span class="token keyword">join</span> ods_order_detail d <span class="token keyword">on</span> i<span class="token punctuation">.</span>id <span class="token operator">=</span> d<span class="token punctuation">.</span>order_id <span class="token operator">and</span> d<span class="token punctuation">.</span>dt <span class="token operator">&gt;</span> <span class="token string">'2021-09-26'</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><div class="language-json line-numbers-mode"><pre class="language-json"><code><span class="token comment">// 第一条语句非等值连接条件作用于左表，两个表会进行全表扫描</span>
<span class="token punctuation">{</span>
    <span class="token property">&quot;input_tables&quot;</span><span class="token operator">:</span><span class="token punctuation">[</span>
        <span class="token punctuation">{</span><span class="token property">&quot;tablename&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_info&quot;</span><span class="token punctuation">,</span><span class="token property">&quot;tabletype&quot;</span><span class="token operator">:</span><span class="token string">&quot;EXTERNAL_TABLE&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;tablename&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_detail&quot;</span><span class="token punctuation">,</span><span class="token property">&quot;tabletype&quot;</span><span class="token operator">:</span><span class="token string">&quot;EXTERNAL_TABLE&quot;</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token property">&quot;input_partitions&quot;</span><span class="token operator">:</span><span class="token punctuation">[</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_info@dt=2021-09-26&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_info@dt=2021-09-27&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_detail@dt=2021-09-26&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_detail@dt=2021-09-27&quot;</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
<span class="token comment">// 第二条语句非等值连接条件作用于右表，左表进行全表扫描，右表只扫描指定分区</span>
<span class="token punctuation">{</span>
    <span class="token property">&quot;input_tables&quot;</span><span class="token operator">:</span><span class="token punctuation">[</span>
        <span class="token punctuation">{</span><span class="token property">&quot;tablename&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_info&quot;</span><span class="token punctuation">,</span><span class="token property">&quot;tabletype&quot;</span><span class="token operator">:</span><span class="token string">&quot;EXTERNAL_TABLE&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;tablename&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_detail&quot;</span><span class="token punctuation">,</span><span class="token property">&quot;tabletype&quot;</span><span class="token operator">:</span><span class="token string">&quot;EXTERNAL_TABLE&quot;</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token property">&quot;input_partitions&quot;</span><span class="token operator">:</span><span class="token punctuation">[</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_info@dt=2021-09-26&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_info@dt=2021-09-27&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token property">&quot;partitionName&quot;</span><span class="token operator">:</span><span class="token string">&quot;dmc_dw@ods_order_detail@dt=2021-09-27&quot;</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><h2 id="数据倾斜场景及解决方案"><a href="#数据倾斜场景及解决方案" class="header-anchor">#</a> 数据倾斜场景及解决方案</h2> <p>MapReduce/Hive 数据倾斜产生阶段/场景：</p> <ul><li>Map：当使用 GZIP 等不支持分片操作的压缩方式时，不可分片的超大文件只被一个 Map 处理</li> <li>Reduce：当任务中处理大量相同 key 数据时，相同 key 数据会被 hash 到同一个 Reduce 处理</li></ul> <p>MapReduce/Hive 数据倾斜解决方案：</p> <ul><li>大量空值分布进行 join 操作：
<ul><li>场景：大量空值进行 join 操作后，产生 Shuffle 阶段，空值全被 hash 到同一个 Reduce</li> <li>方案：不进行空值 join 操作（空值过滤）；对空值进行预处理随机赋值（空值转换）</li></ul></li> <li>不同数据类型字段进行 join 操作：
<ul><li>场景：int/string，默认按照 int 进行 hash 操作，则 string 全被 hash 到同一个 Reduce</li> <li>方案：将不同数据类型字段转为同一类型：将 int 转为 string</li></ul></li> <li>不可分片超大文件：
<ul><li>场景：当某个超大文件使用不支持分片操作的压缩方式进行压缩后，该文件只被一个 Map 处理</li> <li>方案：使用支持文件分片操作的压缩方式：bzip2/zip</li></ul></li> <li>表连接：
<ul><li>场景：表连接的键存在数据倾斜，则 Shuffle 阶段进行 hash 操作会引起数据倾斜</li> <li>方案：将倾斜数据存到分布式缓存并分发到各个 Map 所在节点，在 Map 阶段完成连接（MapJoin，适用于小表 JOIN 大表场景），避免 Shuffle，从而避免数据倾斜</li></ul></li></ul> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token comment">-- 一个解决数据倾斜的 SQL 语句</span>
<span class="token keyword">SELECT</span> 
  a<span class="token punctuation">.</span><span class="token keyword">Key</span> <span class="token punctuation">,</span> <span class="token function">SUM</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>Cnt<span class="token punctuation">)</span> <span class="token keyword">AS</span> Cnt 
<span class="token keyword">FROM</span> <span class="token punctuation">(</span> 
  <span class="token keyword">SELECT</span> 
    <span class="token keyword">Key</span> <span class="token punctuation">,</span> <span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> Cnt 
  <span class="token comment">-- key 对应的数据存在数据倾斜，key=KEY001 的数据占了整份数据的 90%，直接针对 key 进行分组肯定会出现数据倾斜</span>
  <span class="token keyword">FROM</span> TableName <span class="token keyword">GROUP</span> <span class="token keyword">BY</span> <span class="token keyword">Key</span><span class="token punctuation">,</span> 
  <span class="token keyword">CASE</span> 
    <span class="token comment">-- 先把 key=KEY001 的数据打散，分成50份，进行局部聚合，最后再通过外面的 select 进行全局聚合，显著提高计算效率</span>
    <span class="token keyword">WHEN</span> <span class="token keyword">Key</span> <span class="token operator">=</span> <span class="token string">'KEY001'</span> <span class="token keyword">THEN</span> <span class="token keyword">Hash</span><span class="token punctuation">(</span>Random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">50</span> 
    <span class="token keyword">ELSE</span> <span class="token number">0</span> 
  <span class="token keyword">END</span> 
<span class="token punctuation">)</span> a <span class="token keyword">GROUP</span> <span class="token keyword">BY</span> a<span class="token punctuation">.</span><span class="token keyword">Key</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><blockquote><p>Shuffle 阶段堪称性能杀手：最容易引起数据倾斜；Shuffle 过程中会产生大量磁盘 IO、网络 IO 及压缩、解压缩、序列化和反序列化等。这些操作都是严重影响性能</p></blockquote> <h2 id="性能优化"><a href="#性能优化" class="header-anchor">#</a> 性能优化</h2> <h3 id="map-数"><a href="#map-数" class="header-anchor">#</a> Map 数</h3> <p>通常情况下，作业会通过 input 目录产生一个/多个 Map 任务，主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。文件按块进行分片(不足一块按一块算)，一块产生一个 Map 任务，两种情况：</p> <ul><li>合并小文件减少 Map 数：每个小文件(远小于块大小)用一个 Map 完成，而 Map 启动和初始化时间远大于逻辑处理时间，造成资源浪费，同时可执行的 Map 数是受限的</li> <li>复杂大文件增加 Map 数：当 Map 任务逻辑复杂，执行非常慢时，可以考虑增加 Map 数，来使得每个 Map 处理的数据量减少，从而提高任务的执行效率</li></ul> <p>根据实际情况，控制 Map 数量需要遵循两个原则：</p> <ul><li>拆分大文件：使大数据量利用合适的 Map 数</li> <li>合并小文件：使单个 Map 任务处理合适的数据量</li></ul> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 根据 maxcomputeSliteSize(Math.max(minSize, Math.min(maxSize, blocksize)))=blocksize=128M 调整 maxSize 最大值，</span>
<span class="token comment"># 只需 maxSize 最大值 &lt; blocksize 即可增加 Map 数</span>
<span class="token builtin class-name">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">;</span>  <span class="token comment"># 设置最大切片值为 100 个字节</span>

<span class="token comment"># Map 输入前进行小文件合并</span>
<span class="token builtin class-name">set</span> hive.input.format<span class="token operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat<span class="token punctuation">;</span>  <span class="token comment"># 执行 Map 前进行小文件合并</span>

<span class="token comment"># 设置 Map 输出和 Reduce 输出结束后进行合并的相关参数：  </span>
<span class="token builtin class-name">set</span> hive.merge.mapfiles<span class="token operator">=</span>true<span class="token punctuation">;</span>     <span class="token comment"># 设置 Map 端输出进行合并，默认 true  </span>
<span class="token builtin class-name">set</span> hive.merge.mapredfiles<span class="token operator">=</span>true<span class="token punctuation">;</span>  <span class="token comment"># 设置 Reduce 端输出进行合并，默认 false</span>
<span class="token builtin class-name">set</span> hive.merge.size.per.task<span class="token operator">=</span><span class="token number">256</span>*1000*1000<span class="token punctuation">;</span> <span class="token comment"># 设置合并文件的大小</span>
<span class="token builtin class-name">set</span> hive.merge.smallfiles.avgsize<span class="token operator">=</span><span class="token number">16000000</span><span class="token punctuation">;</span> <span class="token comment"># 当输出文件的平均大小小于该值时，启动一个独立 MapReduce 任务进行文件 merge</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><h3 id="reduce-数"><a href="#reduce-数" class="header-anchor">#</a> Reduce 数</h3> <ul><li>过多地启动和初始化 Reduce 也会消耗时间和资源</li> <li>有多少个 Reduce，就会有多少个输出文件，如果生成了很多小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题</li></ul> <p>在设置 Reduce 个数的时候也需要考虑这两个原则：</p> <ul><li>处理大数据量利用合适的 Reduce 数</li> <li>使单个 Reduce 任务处理数据量大小要合适</li></ul> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">set</span> hive.exec.reducers.bytes.per.reducer<span class="token operator">=</span><span class="token number">256123456</span>      <span class="token comment"># 每个 Reduce 处理的数据量，默认 1 G</span>
<span class="token builtin class-name">set</span> hive.exec.reducers.max<span class="token operator">=</span><span class="token number">1009</span>     <span class="token comment"># 每个任务最大的 Reduce 数，默认为 1009</span>
<span class="token comment"># 第一种：Hive 默认计算 Reducer 数的公式，参数1：每个 Reduce 处理的最大数据量 参数2：每个任务最大 Reduce 数量</span>
<span class="token assign-left variable">N</span><span class="token operator">=</span>min<span class="token punctuation">(</span>参数2，总输入数据量/参数1<span class="token punctuation">)</span> <span class="token comment"># 如果 Reduce 输入（map的输出）总大小不超过 1 G，那么只会有一个 Reduce 任务</span>

<span class="token comment"># 第二种：在 mapred-default.xml 文件中修改，设置每个 Job Reduce 个数</span>
<span class="token builtin class-name">set</span> mapreduce.job.reduces <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><blockquote><p>只有一个 Reduce 任务的情况：</p> <ul><li>Reduce 输入数据量总大小不超过参数 hive.exec.reducers.bytes.per.reduce(1 G)</li> <li>聚合运算时没有使用 group by 汇总（如 count(1)）、用了 order by 或笛卡尔积</li></ul></blockquote> <h3 id="fetch-抓取"><a href="#fetch-抓取" class="header-anchor">#</a> Fetch 抓取</h3> <p>Fetch 抓取：指 Hive 中对某些情况的查询（非全部）可以不必使用 MapReduce 计算</p> <div class="language-xml hive-default.xml.template line-numbers-mode"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hive.fetch.task.conversion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>more<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span> <span class="token comment">&lt;!-- 全局查找、字段查找、limit 查找等都不走 MapReduce --&gt;</span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>
      Expects one of [none, minimal, more].
      Some select queries can be converted to single FETCH task minimizing latency.
      Currently the query should be single sourced not having any subquery and should not have
      any aggregations or distincts (which incurs RS), lateral views and joins.
      0. none : disable hive.fetch.task.conversion
      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
      2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 通过执行计划可以看出无 MR 作业，直接 Fetch Operator</span>
<span class="token number">0</span>: jdbc:hive2://localhost:1000<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span> explain <span class="token keyword">select</span> * from student<span class="token punctuation">;</span>
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: student
          Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">2000</span> Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: s_id <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, s_name <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, s_birth <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>, s_sex <span class="token punctuation">(</span>type: string<span class="token punctuation">)</span>
            outputColumnNames: _col0, _col1, _col2, _col3
            Statistics: Num rows: <span class="token number">1</span> Data size: <span class="token number">2000</span> Basic stats: COMPLETE Column stats: NONE
            ListSink
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token punctuation">[</span>root@dmcdw-1 /<span class="token punctuation">]</span><span class="token comment"># beeline -u jdbc:hive2://localhost:10000 -n hue -p hue</span>
<span class="token number">0</span>: jdbc:hive2://localhost:1000<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span> <span class="token builtin class-name">set</span> hive.fetch.task.conversion<span class="token operator">=</span>more<span class="token punctuation">;</span> <span class="token comment"># more：开启 Fetch 抓取，以下语句不走 MapReduce</span>
<span class="token number">0</span>: jdbc:hive2://localhost:1000<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span> <span class="token keyword">select</span> s_name, s_sex from student limit <span class="token number">3</span><span class="token punctuation">;</span>
<span class="token number">0</span>: jdbc:hive2://localhost:1000<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span> <span class="token builtin class-name">set</span> hive.fetch.task.conversion<span class="token operator">=</span>none<span class="token punctuation">;</span> <span class="token comment"># none：不开启 Fetch 抓取，以上语句走 MapReduce</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><img src="/images/kr/bigdata/hive/%E4%B8%8D%E8%B5%B0-MR-%E8%80%97%E6%97%B6-0.1-%E7%A7%92.png" alt=""> <img src="/images/kr/bigdata/hive/%E8%B5%B0-MR-%E8%80%97%E6%97%B6-13.9-%E7%A7%92.png" alt=""></p> <h3 id="本地模式"><a href="#本地模式" class="header-anchor">#</a> 本地模式</h3> <p>大多数 Hadoop Job 是需要 Hadoop 提供的完整可扩展性来处理大数据集。不过，有时 Hive 输入数据量是非常小，在这种情况下，为查询触发执行任务消耗的时间可能会比实际 Job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务，对于小数据集，执行时间可以明显被缩短。</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 开启 local mr，当数据超过最大输入数据量 或者 超过最大输入文件数，不会走 local mr</span>
<span class="token builtin class-name">set</span> hive.exec.mode.local.auto<span class="token operator">=</span>true<span class="token punctuation">;</span> <span class="token comment"># 开启本地模式，让 Hive 在适当时自动启动这个优化，默认 false</span>
<span class="token comment"># 设置本地模式最大输入数据量，当输入数据量小于这个值时采用 local mr 方式，默认 134217728(128 M)</span>
<span class="token builtin class-name">set</span> hive.exec.mode.local.auto.inputbytes.max<span class="token operator">=</span><span class="token number">51234560</span><span class="token punctuation">;</span>
<span class="token comment"># 设置本地模式最大输入文件数，当输入文件数小于这个值时采用 local mr 方式，默认 4</span>
<span class="token builtin class-name">set</span> hive.exec.mode.local.auto.input.files.max<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>当一个 Job 满足如下条件才能真正使用本地模式：</p> <ul><li>job的输入数据大小必须小于最大输入数据量</li> <li>job的 map 数必须小于最大输入文件数</li> <li>job的 reduce 数必须为0或者1</li></ul> <p><img src="/images/kr/bigdata/hive/%E5%85%B3%E9%97%AD%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F-%E8%80%97%E6%97%B6-45.5-%E7%A7%92.png" alt=""> <img src="/images/kr/bigdata/hive/%E5%BC%80%E5%90%AF%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F-%E8%80%97%E6%97%B6-1.8-%E7%A7%92.png" alt=""></p> <h3 id="sql-优化"><a href="#sql-优化" class="header-anchor">#</a> SQL 优化</h3> <h4 id="union-all"><a href="#union-all" class="header-anchor">#</a> union all</h4> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token comment">--  通过执行计划可以看出该 SQL 对表 student 进行了两次同一字段分组</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">type</span><span class="token punctuation">)</span> <span class="token keyword">select</span> s_sex<span class="token punctuation">,</span> <span class="token function">max</span><span class="token punctuation">(</span>s_birth<span class="token punctuation">)</span> <span class="token keyword">as</span> s_birth<span class="token punctuation">,</span> <span class="token string">'max'</span> <span class="token keyword">as</span> <span class="token keyword">type</span> <span class="token keyword">from</span> student <span class="token keyword">group</span> <span class="token keyword">by</span> s_sex
<span class="token keyword">union</span> <span class="token keyword">all</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">type</span><span class="token punctuation">)</span> <span class="token keyword">select</span> s_sex<span class="token punctuation">,</span> <span class="token function">min</span><span class="token punctuation">(</span>s_birth<span class="token punctuation">)</span> <span class="token keyword">as</span> s_birth<span class="token punctuation">,</span> <span class="token string">'min'</span> <span class="token keyword">as</span> <span class="token keyword">type</span> <span class="token keyword">from</span> student <span class="token keyword">group</span> <span class="token keyword">by</span> s_sex<span class="token punctuation">;</span>
<span class="token comment">--开启动态分区</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token operator">=</span>nonstrict<span class="token punctuation">;</span>
<span class="token comment">-- from ... insert into ... ： 使用一张表，可以进行多次插入操作</span>
<span class="token keyword">from</span> student 
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">type</span><span class="token punctuation">)</span> <span class="token keyword">select</span> s_sex<span class="token punctuation">,</span> <span class="token function">max</span><span class="token punctuation">(</span>s_birth<span class="token punctuation">)</span> <span class="token keyword">as</span> s_birth<span class="token punctuation">,</span><span class="token string">'max'</span> <span class="token keyword">as</span> <span class="token keyword">type</span> <span class="token keyword">group</span> <span class="token keyword">by</span> s_sex
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> stu <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">type</span><span class="token punctuation">)</span> <span class="token keyword">select</span> s_sex<span class="token punctuation">,</span> <span class="token function">min</span><span class="token punctuation">(</span>s_birth<span class="token punctuation">)</span> <span class="token keyword">as</span> s_birth<span class="token punctuation">,</span><span class="token string">'min'</span> <span class="token keyword">as</span> <span class="token keyword">type</span> <span class="token keyword">group</span> <span class="token keyword">by</span> s_sex<span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><h4 id="count-distinct"><a href="#count-distinct" class="header-anchor">#</a> COUNT(DISTINCT)</h4> <p>数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个 Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换，但是需要注意 GROUP BY 造成的数据倾斜问题</p> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> s_age <span class="token keyword">from</span> stu <span class="token keyword">group</span> <span class="token keyword">by</span> s_age<span class="token punctuation">)</span> b<span class="token punctuation">;</span>   <span class="token comment">-- 大数据量：先 GROUP BY 再 COUNT 的方式</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> s_age<span class="token punctuation">)</span> <span class="token keyword">from</span> stu<span class="token punctuation">;</span>  <span class="token comment">-- 小数据量：直接 count(distinct)</span>
<span class="token comment">-- 按年龄去重，由于年龄枚举值是有限的</span>
<span class="token comment">-- 如果转化成 MapReduce，在 Map 阶段，每个 Map 会对 s_age 去重，最终得到 Reduce 数据量也就是 Map 数量 * s_age 枚举值个数</span>
<span class="token comment">-- distinct 命令会在内存中构建一个 hashtable，查找去重的时间复杂度是 O(1)</span>
<span class="token comment">-- group by 在不同版本间变动比较大，有的版本会用构建 hashtable 形式去重，有的版本会通过排序的方式。排序最优时间复杂度无法到 O(1)</span>
<span class="token comment">-- 另外，第一种方式(group by)去重会转化为两个任务，会消耗更多的磁盘网络 I/O 资源</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><blockquote><p>Hive 3.0 中新增了 count(distinct) 优化，通过配置 hive.optimize.countdistinct，即使真的出现数据倾斜也可以自动优化，自动改变 SQL 执行逻辑</p></blockquote> <h4 id="group-by"><a href="#group-by" class="header-anchor">#</a> Group By</h4> <p>默认情况下，Map 阶段同一 Key 数据分发给同一个 Reduce 处理，当一个 Key 数据过大时就发生 Reduce 数据倾斜
<img src="/images/kr/bigdata/hive/Group-By-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.bmp" alt=""></p> <p>一种优化方式：并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">set</span> hive.map.aggr <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>                           <span class="token comment"># 开启 Map 端聚合参数设置</span>
<span class="token builtin class-name">set</span> hive.groupby.mapaggr.checkinterval <span class="token operator">=</span> <span class="token number">100000</span><span class="token punctuation">;</span>    <span class="token comment"># 在 Map 端进行聚合操作的条目数目</span>
<span class="token builtin class-name">set</span> hive.groupby.skewindata <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>                 <span class="token comment"># 有数据倾斜时进行负载均衡（默认 false）</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>当 hive.groupby.skewindata = true 时，生成的查询计划会有两个 MR Job：</p> <ul><li>第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同 Reduce 中，从而达到负载均衡的目的</li> <li>第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作</li></ul> <h4 id="小表-join-大表-mapjoin"><a href="#小表-join-大表-mapjoin" class="header-anchor">#</a> 小表 JOIN 大表(MapJoin)</h4> <p>将 key 相对分散且数据量小的表放在 JOIN 左边，可以使用 MapJoin 让小表先加载进内存，在 Map 端完成 JOIN</p> <blockquote><p>新版 Hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化，小表放在左边和右边已经没有区别</p></blockquote> <p><img src="/images/kr/bigdata/hive/MapJoin.png" alt=""></p> <p>MapJoin：在 Map 阶段将小表读入内存，顺序扫描大表完成 Join，MapJoin 分两个阶段：</p> <ol><li>通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对 HashTableFiles 进行压缩</li> <li>MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务</li></ol> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">set</span> hive.auto.convert.join <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment"># 设置自动选择 MapJoin，默认为 true</span>
<span class="token builtin class-name">set</span> hive.mapjoin.smalltable.filesize <span class="token operator">=</span> <span class="token number">25000000</span><span class="token punctuation">;</span>   <span class="token comment"># 大表小表的阈值设置（默认 25M 以下认为是小表），小表加载进内存</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="行列过滤"><a href="#行列过滤" class="header-anchor">#</a> 行列过滤</h4> <ul><li>列处理：在 SELECT 中，只拿需要的列，如果有分区，尽量使用分区过滤，少用 SELECT *</li> <li>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤</li></ul> <h3 id="数据格式优化"><a href="#数据格式优化" class="header-anchor">#</a> 数据格式优化</h3> <p>Hive 提供了多种数据存储组织格式，不同格式对程序的运行效率也会有极大的影响</p> <h3 id="小文件过多优化"><a href="#小文件过多优化" class="header-anchor">#</a> 小文件过多优化</h3> <h4 id="产生"><a href="#产生" class="header-anchor">#</a> 产生？</h4> <ul><li>直接向表中 insert 数据：insert into table values ...，每次插入时都会产生一个文件（不管插入数据量有多大）</li> <li>通过 load 方式加载数据：加载的文件数据源包含大量小文件</li> <li>通过查询方式加载数据：insert overwrite table ... select ...</li> <li>动态分区插入数据，产生大量小文件，从而导致 Map 数量剧增</li></ul> <blockquote><p>insert 导入数据时会启动 MR，MR 中 Reduce 有多少个就输出多少个文件，则文件数量 = ReduceTask 数量 * 分区数；
也有很多简单任务没有 Reduce，只有 Map 阶段，则文件数量 = MapTask 数量 * 分区数
每执行一次 insert 时 Hive 中至少产生一个文件，因为 insert 导入时至少会有一个 MapTask</p></blockquote> <h4 id="影响"><a href="#影响" class="header-anchor">#</a> 影响？</h4> <ul><li>每个小文件启动一个 Map 任务来完成，而 Map 任务启动和初始化时间远远大于逻辑处理时间，就会造成很大的资源浪费</li> <li>HDFS 不适合存储大量小文件，过多小文件会占用 NameNode 大量内存，这样 NameNode 内存容量严重制约了集群的扩展</li></ul> <h4 id="解决"><a href="#解决" class="header-anchor">#</a> 解决？</h4> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">alter</span> <span class="token keyword">table</span> table_name <span class="token punctuation">[</span><span class="token keyword">partition</span><span class="token punctuation">(</span>part_field<span class="token operator">=</span>part_value<span class="token punctuation">)</span><span class="token punctuation">]</span> concatenate<span class="token punctuation">;</span> <span class="token comment">-- 使用 hive concatenate 命令，自动合并小文件</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><ol><li>concatenate 命令只支持 RCFILE 和 ORC 文件类型</li> <li>concatenate 命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令</li> <li>当多次 concatenate 后文件数量不变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 有关，可设定每个文件最小大小</li></ol></blockquote> <p>通过参数调节，设置 Map/Reduce 端的相关参数，以减少 Map/Reduce 数量：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 设置 Map 输入前进行合并小文件的相关参数：</span>
<span class="token builtin class-name">set</span> mapred.max.split.size<span class="token operator">=</span><span class="token number">256000000</span><span class="token punctuation">;</span>    <span class="token comment"># 每个 Map 最大输入大小(这个值决定了合并后文件的数量)    </span>
<span class="token builtin class-name">set</span> mapred.min.split.size.per.node<span class="token operator">=</span><span class="token number">100000000</span><span class="token punctuation">;</span>   <span class="token comment"># 一个节点上 split 的至少的大小(这个值决定了多个 DataNode 上的文件是否需要合并) </span>
<span class="token builtin class-name">set</span> mapred.min.split.size.per.rack<span class="token operator">=</span><span class="token number">100000000</span><span class="token punctuation">;</span>   <span class="token comment"># 一个交换机下 split 的至少的大小(这个值决定了多个交换机上的文件是否需要合并) </span>
<span class="token comment"># CombineHiveInputFormat 底层是 Hadoop CombineFileInputFormat 方法</span>
<span class="token comment"># 此方法是在 Mapper 中将多个文件合成一个 split 作为输入</span>
<span class="token builtin class-name">set</span> hive.input.format<span class="token operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat<span class="token punctuation">;</span>  <span class="token comment"># 执行 Map 前进行小文件合并</span>

<span class="token comment"># 设置 Map 输出和 Reduce 输出结束后进行合并的相关参数：  </span>
<span class="token builtin class-name">set</span> hive.merge.mapfiles<span class="token operator">=</span>true<span class="token punctuation">;</span>     <span class="token comment"># 设置 Map 端输出进行合并，默认 true  </span>
<span class="token builtin class-name">set</span> hive.merge.mapredfiles<span class="token operator">=</span>true<span class="token punctuation">;</span>  <span class="token comment"># 设置 Reduce 端输出进行合并，默认 false</span>
<span class="token builtin class-name">set</span> hive.merge.size.per.task<span class="token operator">=</span><span class="token number">256</span>*1000*1000<span class="token punctuation">;</span> <span class="token comment"># 设置合并文件的大小</span>
<span class="token builtin class-name">set</span> hive.merge.smallfiles.avgsize<span class="token operator">=</span><span class="token number">16000000</span><span class="token punctuation">;</span> <span class="token comment"># 当输出文件的平均大小小于该值时，启动一个独立 MapReduce 任务进行文件 merge</span>

<span class="token comment"># 启用压缩</span>
<span class="token builtin class-name">set</span> hive.exec.compress.output<span class="token operator">=</span>true<span class="token punctuation">;</span>         <span class="token comment"># Hive 的查询结果输出是否进行压缩</span>
<span class="token builtin class-name">set</span> mapreduce.output.fileoutputformat.compress<span class="token operator">=</span>true<span class="token punctuation">;</span>    <span class="token comment"># MapReduce Job 的结果输出是否使用压缩</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># Reduce 的个数决定了输出的文件的个数，所以可以调整 Reduce 的个数控制 Hive 表的文件数量</span>
<span class="token comment"># hive 中的分区函数 distribute by 正好是控制 MR 中 partition 分区的</span>
<span class="token comment"># 然后通过设置 reduce 的数量，结合分区函数让数据均衡的进入每个 reduce 即可</span>
<span class="token comment"># 设置reduce的数量有两种方式，第一种是直接设置 reduce 个数</span>
<span class="token builtin class-name">set</span> mapreduce.job.reduces<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">;</span>
<span class="token comment"># 第二种是设置每个 reduce 的大小，Hive 会根据数据总大小猜测确定一个 reduce 个数</span>
<span class="token builtin class-name">set</span> hive.exec.reducers.bytes.per.reducer<span class="token operator">=</span><span class="token number">5120000000</span><span class="token punctuation">;</span> <span class="token comment"># 默认是1G，设置为5G</span>

distribute by rand<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment"># 若设置reduce数量为10，则使用 rand()，随机生成一个数 x % 10 ，这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>使用 Hadoop 的 archive 将小文件归档：Hadoop Archive 简称 HAR，是一个高效地将小文件放入 HDFS 块中的文件存档工具，它能够将多个小文件打包成一个 HAR 文件，这样在减少 NameNode 内存使用的同时，仍然允许对文件进行透明的访问。</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">set</span> hive.archive.enabled<span class="token operator">=</span>true<span class="token punctuation">;</span>                  <span class="token comment"># 用来控制归档是否可用</span>
<span class="token builtin class-name">set</span> hive.archive.har.parentdir.settable<span class="token operator">=</span>true<span class="token punctuation">;</span>   <span class="token comment"># 通知 Hive 在创建归档时是否可以设置父目录</span>
<span class="token builtin class-name">set</span> har.partfile.size<span class="token operator">=</span><span class="token number">1099511627776</span><span class="token punctuation">;</span>            <span class="token comment"># 控制需要归档文件的大小</span>

<span class="token comment"># 归档的分区可以查看不能 insert overwrite，必须先 unarchive</span>
ALTER TABLE A ARCHIVE PARTITION<span class="token punctuation">(</span>dt<span class="token operator">=</span><span class="token string">'2020-12-24'</span>, <span class="token assign-left variable">hr</span><span class="token operator">=</span><span class="token string">'12'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment"># 使用以下命令进行归档</span>
ALTER TABLE A UNARCHIVE PARTITION<span class="token punctuation">(</span>dt<span class="token operator">=</span><span class="token string">'2020-12-24'</span>, <span class="token assign-left variable">hr</span><span class="token operator">=</span><span class="token string">'12'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment"># 对已归档的分区恢复为原文件</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="严格模式-strict"><a href="#严格模式-strict" class="header-anchor">#</a> 严格模式(strict)</h3> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">set</span> hive.mapred.mode<span class="token operator">=</span>strict<span class="token punctuation">;</span>        <span class="token comment"># 使用严格模式可以禁止 3 种类型的查询，默认 nonstrict</span>
<span class="token builtin class-name">set</span> hive.strict.checks.no.partition.filter<span class="token operator">=</span>true<span class="token punctuation">;</span> <span class="token comment"># 对于分区表，不加分区字段过滤条件，不能执行</span>
<span class="token comment"># 对于 order by 语句，必须使用 limit 语句，因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reducer 中进行处理</span>
<span class="token builtin class-name">set</span> hive.strict.checks.orderby.no.limit<span class="token operator">=</span>true<span class="token punctuation">;</span>
<span class="token builtin class-name">set</span> hive.strict.checks.cartesian.product<span class="token operator">=</span>true<span class="token punctuation">;</span>   <span class="token comment"># 限制笛卡尔积的查询（join 时不使用 on，而使用 where）</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="动态分区调整"><a href="#动态分区调整" class="header-anchor">#</a> 动态分区调整</h3> <p>关系型数据库中，对分区表 Insert 时，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区 (Dynamic Partition)，参数：</p> <ul><li>hive.exec.dynamic.partition=true：开启动态分区功能，默认开启</li> <li>hive.exec.dynamic.partition.mode=nonstrict：动态分区模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict：表示允许所有分区字段都可以使用动态分区</li> <li>hive.exec.max.dynamic.partitions=1000：在所有执行 MR 节点上，最大一共可以创建多少个动态分区</li> <li>hive.exec.max.dynamic.partitions.pernode=100：在每个执行 MR 节点上，最大可以创建多少个动态分区</li> <li>hive.exec.max.created.files=100000：整个 MR 中，最大可以创建多少个 HDFS 文件</li> <li>hive.error.on.empty.partition=false：当有空分区生成时，是否抛出异常</li></ul> <div class="language-xml hdfs-site.xml line-numbers-mode"><pre class="language-xml"><code><span class="token comment">&lt;!-- 控制 DataNode 一次可以打开的文件个数 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.datanode.max.xcievers<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>8192<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token comment">-- 配置：以第一个表的分区规则，来对应第二个表的分区规则，将第一个表的所有分区，全部拷贝到第二个表中来，</span>
<span class="token comment">-- 第二个表在加载数据的时候，不需要指定分区了，直接用第一个表的分区即可</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> ori_partitioned<span class="token punctuation">(</span>id <span class="token keyword">bigint</span><span class="token punctuation">,</span> <span class="token keyword">time</span> <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span> 
    PARTITIONED <span class="token keyword">BY</span> <span class="token punctuation">(</span>p_time <span class="token keyword">bigint</span><span class="token punctuation">)</span> <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> ori_partitioned_target<span class="token punctuation">(</span>id <span class="token keyword">bigint</span><span class="token punctuation">,</span> <span class="token keyword">time</span> <span class="token keyword">bigint</span><span class="token punctuation">,</span> uid string<span class="token punctuation">,</span> keyword string<span class="token punctuation">,</span> url_rank <span class="token keyword">int</span><span class="token punctuation">,</span> click_num <span class="token keyword">int</span><span class="token punctuation">,</span> click_url string<span class="token punctuation">)</span> 
    PARTITIONED <span class="token keyword">BY</span> <span class="token punctuation">(</span>p_time STRING<span class="token punctuation">)</span> <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>

<span class="token comment">-- 动态分区：在 SELECT 子句的最后几个字段，必须对应前面 PARTITION 中指定的分区字段，包括顺序</span>
<span class="token keyword">INSERT</span> overwrite <span class="token keyword">TABLE</span> ori_partitioned_target <span class="token keyword">PARTITION</span> <span class="token punctuation">(</span>p_time<span class="token punctuation">)</span>
    <span class="token keyword">SELECT</span> id<span class="token punctuation">,</span> <span class="token keyword">time</span><span class="token punctuation">,</span> uid<span class="token punctuation">,</span> keyword<span class="token punctuation">,</span> url_rank<span class="token punctuation">,</span> click_num<span class="token punctuation">,</span> click_url<span class="token punctuation">,</span> p_time <span class="token keyword">FROM</span> ori_partitioned<span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h3 id="并行执行优化"><a href="#并行执行优化" class="header-anchor">#</a> 并行执行优化</h3> <p>Hive 会将一个查询转化成一个或多个阶段(Stage)：MapReduce 阶段、抽样阶段、合并阶段、limit 阶段等，默认情况下，Hive 一次只会执行一个阶段。当一个作业的多个阶段并非完全相互依赖时，可以并行执行，缩短执行时间。</p> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token comment">-- 并行执行在集群资源空闲时能够提高集群资源利用率</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment">-- 开启并行执行</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>thread<span class="token punctuation">.</span>number<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">;</span>  <span class="token comment">-- 同一个 sql 允许最大并行度，默认 8</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="jvm-重用"><a href="#jvm-重用" class="header-anchor">#</a> JVM 重用</h3> <p>JVM 重用是 Hadoop 调优参数的内容，其对 Hive 的性能具有非常大的影响。用于很难避免小文件的场景或者 task 特别多的场景，这类场景大多数执行时间都很短，因为 Hadoop 默认配置通常是使用派生 JVM 来执行 Map/Reduce 任务，在 Hive 执行 MapReduce 任务，JVM 的启动过程会造成很大的开销，尤其是 job 有成千上万个 task 任务时。JVM 重用可以使得 JVM 实例在同一个 job 中重新使用 N 次</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">set</span> mapred.job.reuse.jvm.num.tasks<span class="token operator">=</span>N<span class="token punctuation">;</span> <span class="token comment"># N 为重用个数</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-xml mapred-site.xml line-numbers-mode"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>mapreduce.job.jvm.numtasks<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>10<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span> <span class="token comment">&lt;!-- 重用个数 --&gt;</span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>How many tasks to run per jvm. If set to -1, there is no limit.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>缺点：开启 JVM 重用将一直占用使用到的 JVM 实例，以便进行重用，直到任务完成后才能释放；若某个作业中有某几个 Reduce task 执行时间要比其他 Reduce task 消耗时间更多时，那么占用的 JVM 实例就会一直空闲着却无法被其他作业使用，直到所有的 task 都结束了才会释放。</p> <h3 id="推测执行优化"><a href="#推测执行优化" class="header-anchor">#</a> 推测执行优化</h3> <p>在分布式集群环境下，因负载不均衡或资源分布不均等原因，会造成同一个作业多个任务之间运行速度不一致，有些任务运行速度可能明显慢于其他任务，则这些任务会拖慢作业的整体执行进度。
为了避免这种情况发生，Hadoop 采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p> <div class="language-xml mapred-site.xml line-numbers-mode"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>mapreduce.map.speculative<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>mapreduce.reduce.speculative<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span>speculative<span class="token punctuation">.</span>execution<span class="token operator">=</span><span class="token boolean">true</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>缺点：若因输入数据量很大而需要执行长时间的 Map/Reduce task 时，那么启动推测执行会造成资源浪费和压力是非常巨大的</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新: </span> <span class="time">2022/6/8</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/kr/bigdata/hive/分布式数据仓库-Hive.html" class="prev">
          分布式数据仓库 Hive
        </a></span> <span class="next"><a href="/kr/bigdata/hive/HiveSQL-编译过程.html">
          Hive 工作原理：Hive SQL 编译过程
        </a>
        →
      </span></p></div> </main></div> <aside class="page-sidebar"> <div class="page-side-toolbar"><div class="option-box-toc-fixed"><div class="toc-container-sidebar"><div class="pos-box"><div class="icon-arrow"></div> <div class="scroll-box" style="max-height:650px"><div style="font-weight:bold;text-align:center;">Hive SQL 执行计划|数据倾斜|性能优化</div> <hr> <div class="toc-box"><ul class="toc-sidebar-links"><li><a href="/kr/bigdata/hive/Hive-SQL.html#执行计划" class="toc-sidebar-link">执行计划</a><ul class="toc-sidebar-sub-headers"><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#explaint" class="toc-sidebar-link">explaint</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#explain-dependency" class="toc-sidebar-link">explain dependency</a></li></ul></li><li><a href="/kr/bigdata/hive/Hive-SQL.html#数据倾斜场景及解决方案" class="toc-sidebar-link">数据倾斜场景及解决方案</a><ul class="toc-sidebar-sub-headers"></ul></li><li><a href="/kr/bigdata/hive/Hive-SQL.html#性能优化" class="toc-sidebar-link">性能优化</a><ul class="toc-sidebar-sub-headers"><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#map-数" class="toc-sidebar-link">Map 数</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#reduce-数" class="toc-sidebar-link">Reduce 数</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#fetch-抓取" class="toc-sidebar-link">Fetch 抓取</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#本地模式" class="toc-sidebar-link">本地模式</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#sql-优化" class="toc-sidebar-link">SQL 优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#数据格式优化" class="toc-sidebar-link">数据格式优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#小文件过多优化" class="toc-sidebar-link">小文件过多优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#严格模式-strict" class="toc-sidebar-link">严格模式(strict)</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#动态分区调整" class="toc-sidebar-link">动态分区调整</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#并行执行优化" class="toc-sidebar-link">并行执行优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#jvm-重用" class="toc-sidebar-link">JVM 重用</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#推测执行优化" class="toc-sidebar-link">推测执行优化</a></li></ul></li></ul></div></div></div></div></div> <div class="option-box-toc-over"><img src="/images/system/toc.png" class="nozoom"> <span class="show-txt">目录</span> <div class="toc-container"><div class="pos-box"><div class="icon-arrow"></div> <div class="scroll-box" style="max-height:550px"><div style="font-weight:bold;text-align:center;">Hive SQL 执行计划|数据倾斜|性能优化</div> <hr> <div class="toc-box"><ul class="toc-sidebar-links"><li><a href="/kr/bigdata/hive/Hive-SQL.html#执行计划" class="toc-sidebar-link">执行计划</a><ul class="toc-sidebar-sub-headers"><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#explaint" class="toc-sidebar-link">explaint</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#explain-dependency" class="toc-sidebar-link">explain dependency</a></li></ul></li><li><a href="/kr/bigdata/hive/Hive-SQL.html#数据倾斜场景及解决方案" class="toc-sidebar-link">数据倾斜场景及解决方案</a><ul class="toc-sidebar-sub-headers"></ul></li><li><a href="/kr/bigdata/hive/Hive-SQL.html#性能优化" class="toc-sidebar-link">性能优化</a><ul class="toc-sidebar-sub-headers"><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#map-数" class="toc-sidebar-link">Map 数</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#reduce-数" class="toc-sidebar-link">Reduce 数</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#fetch-抓取" class="toc-sidebar-link">Fetch 抓取</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#本地模式" class="toc-sidebar-link">本地模式</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#sql-优化" class="toc-sidebar-link">SQL 优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#数据格式优化" class="toc-sidebar-link">数据格式优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#小文件过多优化" class="toc-sidebar-link">小文件过多优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#严格模式-strict" class="toc-sidebar-link">严格模式(strict)</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#动态分区调整" class="toc-sidebar-link">动态分区调整</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#并行执行优化" class="toc-sidebar-link">并行执行优化</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#jvm-重用" class="toc-sidebar-link">JVM 重用</a></li><li class="toc-sidebar-sub-header"><a href="/kr/bigdata/hive/Hive-SQL.html#推测执行优化" class="toc-sidebar-link">推测执行优化</a></li></ul></li></ul></div></div></div></div></div></div>  </aside></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.cfd60917.js" defer></script><script src="/assets/js/4.8234885e.js" defer></script><script src="/assets/js/3.1429892b.js" defer></script><script src="/assets/js/26.fc7addb7.js" defer></script>
  </body>
</html>
