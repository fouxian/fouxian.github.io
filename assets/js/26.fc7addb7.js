(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{484:function(s,t,a){"use strict";a.r(t);var n=a(34),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"hive-sql-执行计划-数据倾斜-性能优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-sql-执行计划-数据倾斜-性能优化"}},[s._v("#")]),s._v(" Hive SQL 执行计划|数据倾斜|性能优化")]),s._v(" "),a("h2",{attrs:{id:"执行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#执行计划"}},[s._v("#")]),s._v(" 执行计划")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("EXPLAIN")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("EXTENDED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" DEPENDENCY "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AUTHORIZATION")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" query\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("通过执行计划能了解 SQL 在转换成相应计算引擎的执行逻辑，掌握了执行逻辑也就能更好地把握程序出现的瓶颈点，从而能够实现更有针对性的优化。Hive 集成了 Apache Calcite，使得 Hive 能够基于成本代价来生成执行计划，这种方式能够结合 Hive 元数据信息和 Hive  运行过程收集的各类统计信息推测出一个更为合理的执行计划，Hive  目前所提供的执行计划都是预估。")]),s._v(" "),a("p",[s._v("Hive 执行计划提供的信息：")]),s._v(" "),a("ul",[a("li",[s._v("查看执行计划的基本信息（explain）")]),s._v(" "),a("li",[s._v("查看执行计划的扩展信息（explain extended）")]),s._v(" "),a("li",[s._v("查看 SQL 数据输入依赖的信息（explain dependency）")]),s._v(" "),a("li",[s._v("查看 SQL 操作相关权限的信息（explain authorization）")]),s._v(" "),a("li",[s._v("查看 SQL 的向量化描述信息（explain vectorization）")])]),s._v(" "),a("h3",{attrs:{id:"explaint"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#explaint"}},[s._v("#")]),s._v(" explaint")]),s._v(" "),a("p",[s._v("explaint 执行计划包含两部分：")]),s._v(" "),a("ul",[a("li",[s._v("作业的依赖关系图，即 STAGE DEPENDENCIES")]),s._v(" "),a("li",[s._v("每个作业的详细信息，即 STAGE PLANS")])]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# HQL 语句被转换为由多个 stage(阶段) 组成的序列（有向无环图 DAG），")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 这些 stage 包括：MR stage、元数据存储的 stage、文件系统操作的 stage")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 有 MR 作业")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(": jdbc:hive2://localhost:1000"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("0")]),s._v(">")]),s._v(" explain "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" * from score cluster by s_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nSTAGE DEPENDENCIES:                     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 各个 stage 间的依赖关系")]),s._v("\n  Stage-1 is a root stage               "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# root stage，执行的起始 stage")]),s._v("\n  Stage-0 depends on stages: Stage-1    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 依赖于 stage-1")]),s._v("\n\nSTAGE PLANS:                            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 各个 stage 的执行计划")]),s._v("\n  Stage: Stage-1                        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# stage-1 的执行计划")]),s._v("\n    Map Reduce                          "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# MapReduce 作业的执行计划")]),s._v("\n      Map Operator Tree:                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map 端的执行计划树")]),s._v("\n          TableScan                     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表扫描操作")]),s._v("\n            alias: score                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表名称/别名")]),s._v("\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表统计信息")]),s._v("\n            Select Operator             "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 选取操作")]),s._v("\n              expressions: s_id "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", c_id "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", s_score "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 选取需要查询的表字段名/类型")]),s._v("\n              outputColumnNames: _col0, _col1, _col2        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 输出的列名")]),s._v("\n              Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表统计信息")]),s._v("\n              Reduce Output Operator    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 输出到 Reduce 的操作")]),s._v("\n                key expressions: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map key")]),s._v("\n                "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("sort")]),s._v(" order: +           "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 排序")]),s._v("\n                Map-reduce partition columns: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n                Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n                value expressions: _col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", _col2 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map value")]),s._v("\n      Execution mode: vectorized\n      Reduce Operator Tree:             "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Reduce 端的执行计划树")]),s._v("\n        Select Operator                 "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 选取操作")]),s._v("\n          expressions: KEY.reducesinkkey0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", VALUE._col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", VALUE._col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n          outputColumnNames: _col0, _col1, _col2\n          Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n          File Output Operator          "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 文件输出操作")]),s._v("\n            compressed: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("false")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 是否压缩")]),s._v("\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n            table:\n                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\n  Stage: Stage-0\n    Fetch Operator                      "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 客户端取数据操作")]),s._v("\n      limit: -1\n      Processor Tree:\n        ListSink\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br")])]),a("h4",{attrs:{id:"常见-operator-操作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#常见-operator-操作"}},[s._v("#")]),s._v(" 常见 operator 操作")]),s._v(" "),a("p",[s._v("Map/Reduce 的执行计划中包含常见的 operator 操作：")]),s._v(" "),a("ul",[a("li",[s._v("TableScan 表扫描操作\n"),a("ul",[a("li",[s._v("alias：表名称/别名")]),s._v(" "),a("li",[s._v("Statistics：表统计信息")])])]),s._v(" "),a("li",[s._v("Select Operator 选取操作\n"),a("ul",[a("li",[s._v("expressions：需要查询的表字段名/类型")]),s._v(" "),a("li",[s._v("outputColumnNames：输出的列名称")]),s._v(" "),a("li",[s._v("Statistics：表统计信息")])])]),s._v(" "),a("li",[s._v("Group By Operator 分组聚合操作\n"),a("ul",[a("li",[s._v("aggregations：显示聚合函数信息")]),s._v(" "),a("li",[s._v("mode：聚合模式，值有 hash：随机聚合(hash partition)；partial：局部聚合；final：最终聚合")]),s._v(" "),a("li",[s._v("keys：分组的字段")]),s._v(" "),a("li",[s._v("outputColumnNames：聚合之后输出列名")]),s._v(" "),a("li",[s._v("Statistics：表统计信息，包含分组聚合之后的数据条数，数据大小等")])])]),s._v(" "),a("li",[s._v("Reduce Output Operator 输出到 Reduce 操作\n"),a("ul",[a("li",[s._v("sort order：值为空不排序；+ 正序排序；- 倒序排序；+- 排序的列为两列，第一列为正序，第二列为倒序")])])]),s._v(" "),a("li",[s._v("Filter Operator 过滤操作\n"),a("ul",[a("li",[s._v("predicate：过滤条件")])])]),s._v(" "),a("li",[s._v("Map Join Operator join 操作\n"),a("ul",[a("li",[s._v("condition map：join 方式")]),s._v(" "),a("li",[s._v("keys：join 的条件字段")]),s._v(" "),a("li",[s._v("outputColumnNames：join 完成之后输出的字段")]),s._v(" "),a("li",[s._v("Statistics：join 完成之后生成的数据条数，大小等")])])]),s._v(" "),a("li",[s._v("File Output Operator 文件输出操作\n"),a("ul",[a("li",[s._v("compressed：是否压缩")]),s._v(" "),a("li",[s._v("table：表的信息，包含输入输出文件格式化方式，序列化方式等")])])]),s._v(" "),a("li",[s._v("Fetch Operator 客户端获取数据操作\n"),a("ul",[a("li",[s._v("limit：值为 -1 表示不限制条数，其他值为限制的条数")])])])]),s._v(" "),a("h4",{attrs:{id:"join-会过滤-null-值"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#join-会过滤-null-值"}},[s._v("#")]),s._v(" Join 会过滤 null 值？")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("explain")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("s_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("s_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" o"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("c_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" o"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("s_score "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student s "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" score o "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("s_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" o"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("s_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 通过执行计划的 predicate: s_id is not null (type: boolean) 可以看出在 join 两个表时会过滤 null 值")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 但 left join 和 full join 不会过滤 null 值")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-Join-null%E5%A4%84%E7%90%86.png",alt:""}})]),s._v(" "),a("h4",{attrs:{id:"group-by-会进行排序"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#group-by-会进行排序"}},[s._v("#")]),s._v(" Group by 会进行排序？")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("explain")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("max")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s_score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" score "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- keys: s_id (type: string)： 按照 s_id 分组；sort order: + 正序排序 ")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-group-by-%E6%8E%92%E5%BA%8F.png",alt:""}})]),s._v(" "),a("h3",{attrs:{id:"explain-dependency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#explain-dependency"}},[s._v("#")]),s._v(" explain dependency")]),s._v(" "),a("p",[s._v("explain dependency 用于描述 SQL 数据来源，输出一个 json 格式数据，常用于分析 SQL 数据过滤情况，两部分：")]),s._v(" "),a("ul",[a("li",[s._v("input_partitions：SQL 数据来源表分区，存储的是分区名，若整段 SQL 所有表都是非分区表，则为空")]),s._v(" "),a("li",[s._v("input_tables：SQL 数据来源表，存储的是 Hive 表名")])]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("explain")]),s._v(" dependency "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" ods_order_info i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("left")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" ods_order_detail d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("order_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2021-09-26'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("explain")]),s._v(" dependency "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" ods_order_info i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("left")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" ods_order_detail d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("order_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2021-09-26'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("div",{staticClass:"language-json line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-json"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 第一条语句非等值连接条件作用于左表，两个表会进行全表扫描")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"input_tables"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tablename"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_info"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tabletype"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"EXTERNAL_TABLE"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tablename"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_detail"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tabletype"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"EXTERNAL_TABLE"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"input_partitions"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_info@dt=2021-09-26"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_info@dt=2021-09-27"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_detail@dt=2021-09-26"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_detail@dt=2021-09-27"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 第二条语句非等值连接条件作用于右表，左表进行全表扫描，右表只扫描指定分区")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"input_tables"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tablename"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_info"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tabletype"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"EXTERNAL_TABLE"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tablename"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_detail"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"tabletype"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"EXTERNAL_TABLE"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"input_partitions"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_info@dt=2021-09-26"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_info@dt=2021-09-27"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token property"}},[s._v('"partitionName"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"dmc_dw@ods_order_detail@dt=2021-09-27"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br")])]),a("h2",{attrs:{id:"数据倾斜场景及解决方案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据倾斜场景及解决方案"}},[s._v("#")]),s._v(" 数据倾斜场景及解决方案")]),s._v(" "),a("p",[s._v("MapReduce/Hive 数据倾斜产生阶段/场景：")]),s._v(" "),a("ul",[a("li",[s._v("Map：当使用 GZIP 等不支持分片操作的压缩方式时，不可分片的超大文件只被一个 Map 处理")]),s._v(" "),a("li",[s._v("Reduce：当任务中处理大量相同 key 数据时，相同 key 数据会被 hash 到同一个 Reduce 处理")])]),s._v(" "),a("p",[s._v("MapReduce/Hive 数据倾斜解决方案：")]),s._v(" "),a("ul",[a("li",[s._v("大量空值分布进行 join 操作：\n"),a("ul",[a("li",[s._v("场景：大量空值进行 join 操作后，产生 Shuffle 阶段，空值全被 hash 到同一个 Reduce")]),s._v(" "),a("li",[s._v("方案：不进行空值 join 操作（空值过滤）；对空值进行预处理随机赋值（空值转换）")])])]),s._v(" "),a("li",[s._v("不同数据类型字段进行 join 操作：\n"),a("ul",[a("li",[s._v("场景：int/string，默认按照 int 进行 hash 操作，则 string 全被 hash 到同一个 Reduce")]),s._v(" "),a("li",[s._v("方案：将不同数据类型字段转为同一类型：将 int 转为 string")])])]),s._v(" "),a("li",[s._v("不可分片超大文件：\n"),a("ul",[a("li",[s._v("场景：当某个超大文件使用不支持分片操作的压缩方式进行压缩后，该文件只被一个 Map 处理")]),s._v(" "),a("li",[s._v("方案：使用支持文件分片操作的压缩方式：bzip2/zip")])])]),s._v(" "),a("li",[s._v("表连接：\n"),a("ul",[a("li",[s._v("场景：表连接的键存在数据倾斜，则 Shuffle 阶段进行 hash 操作会引起数据倾斜")]),s._v(" "),a("li",[s._v("方案：将倾斜数据存到分布式缓存并分发到各个 Map 所在节点，在 Map 阶段完成连接（MapJoin，适用于小表 JOIN 大表场景），避免 Shuffle，从而避免数据倾斜")])])])]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 一个解决数据倾斜的 SQL 语句")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" \n  a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Key")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("SUM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Cnt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AS")]),s._v(" Cnt \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v(" \n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Key")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AS")]),s._v(" Cnt \n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- key 对应的数据存在数据倾斜，key=KEY001 的数据占了整份数据的 90%，直接针对 key 进行分组肯定会出现数据倾斜")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" TableName "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Key")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CASE")]),s._v(" \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 先把 key=KEY001 的数据打散，分成50份，进行局部聚合，最后再通过外面的 select 进行全局聚合，显著提高计算效率")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WHEN")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Key")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'KEY001'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("THEN")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Hash")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("Random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50")]),s._v(" \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ELSE")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" \n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("END")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Key")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br")])]),a("blockquote",[a("p",[s._v("Shuffle 阶段堪称性能杀手：最容易引起数据倾斜；Shuffle 过程中会产生大量磁盘 IO、网络 IO 及压缩、解压缩、序列化和反序列化等。这些操作都是严重影响性能")])]),s._v(" "),a("h2",{attrs:{id:"性能优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#性能优化"}},[s._v("#")]),s._v(" 性能优化")]),s._v(" "),a("h3",{attrs:{id:"map-数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#map-数"}},[s._v("#")]),s._v(" Map 数")]),s._v(" "),a("p",[s._v("通常情况下，作业会通过 input 目录产生一个/多个 Map 任务，主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。文件按块进行分片(不足一块按一块算)，一块产生一个 Map 任务，两种情况：")]),s._v(" "),a("ul",[a("li",[s._v("合并小文件减少 Map 数：每个小文件(远小于块大小)用一个 Map 完成，而 Map 启动和初始化时间远大于逻辑处理时间，造成资源浪费，同时可执行的 Map 数是受限的")]),s._v(" "),a("li",[s._v("复杂大文件增加 Map 数：当 Map 任务逻辑复杂，执行非常慢时，可以考虑增加 Map 数，来使得每个 Map 处理的数据量减少，从而提高任务的执行效率")])]),s._v(" "),a("p",[s._v("根据实际情况，控制 Map 数量需要遵循两个原则：")]),s._v(" "),a("ul",[a("li",[s._v("拆分大文件：使大数据量利用合适的 Map 数")]),s._v(" "),a("li",[s._v("合并小文件：使单个 Map 任务处理合适的数据量")])]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 根据 maxcomputeSliteSize(Math.max(minSize, Math.min(maxSize, blocksize)))=blocksize=128M 调整 maxSize 最大值，")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 只需 maxSize 最大值 < blocksize 即可增加 Map 数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.input.fileinputformat.split.maxsize"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置最大切片值为 100 个字节")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map 输入前进行小文件合并")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.input.format"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 执行 Map 前进行小文件合并")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Map 输出和 Reduce 输出结束后进行合并的相关参数：  ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapfiles"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Map 端输出进行合并，默认 true  ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapredfiles"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Reduce 端输出进行合并，默认 false")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.size.per.task"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),s._v("*1000*1000"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置合并文件的大小")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.smallfiles.avgsize"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("16000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 当输出文件的平均大小小于该值时，启动一个独立 MapReduce 任务进行文件 merge")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("h3",{attrs:{id:"reduce-数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reduce-数"}},[s._v("#")]),s._v(" Reduce 数")]),s._v(" "),a("ul",[a("li",[s._v("过多地启动和初始化 Reduce 也会消耗时间和资源")]),s._v(" "),a("li",[s._v("有多少个 Reduce，就会有多少个输出文件，如果生成了很多小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题")])]),s._v(" "),a("p",[s._v("在设置 Reduce 个数的时候也需要考虑这两个原则：")]),s._v(" "),a("ul",[a("li",[s._v("处理大数据量利用合适的 Reduce 数")]),s._v(" "),a("li",[s._v("使单个 Reduce 任务处理数据量大小要合适")])]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.reducers.bytes.per.reducer"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("256123456")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 每个 Reduce 处理的数据量，默认 1 G")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.reducers.max"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1009")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 每个任务最大的 Reduce 数，默认为 1009")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 第一种：Hive 默认计算 Reducer 数的公式，参数1：每个 Reduce 处理的最大数据量 参数2：每个任务最大 Reduce 数量")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("N")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("参数2，总输入数据量/参数1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 如果 Reduce 输入（map的输出）总大小不超过 1 G，那么只会有一个 Reduce 任务")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 第二种：在 mapred-default.xml 文件中修改，设置每个 Job Reduce 个数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.job.reduces "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("blockquote",[a("p",[s._v("只有一个 Reduce 任务的情况：")]),s._v(" "),a("ul",[a("li",[s._v("Reduce 输入数据量总大小不超过参数 hive.exec.reducers.bytes.per.reduce(1 G)")]),s._v(" "),a("li",[s._v("聚合运算时没有使用 group by 汇总（如 count(1)）、用了 order by 或笛卡尔积")])])]),s._v(" "),a("h3",{attrs:{id:"fetch-抓取"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fetch-抓取"}},[s._v("#")]),s._v(" Fetch 抓取")]),s._v(" "),a("p",[s._v("Fetch 抓取：指 Hive 中对某些情况的查询（非全部）可以不必使用 MapReduce 计算")]),s._v(" "),a("div",{staticClass:"language-xml hive-default.xml.template line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-xml"}},[a("code",[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("hive.fetch.task.conversion"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("more"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("\x3c!-- 全局查找、字段查找、limit 查找等都不走 MapReduce --\x3e")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n      Expects one of [none, minimal, more].\n      Some select queries can be converted to single FETCH task minimizing latency.\n      Currently the query should be single sourced not having any subquery and should not have\n      any aggregations or distincts (which incurs RS), lateral views and joins.\n      0. none : disable hive.fetch.task.conversion\n      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n      2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 通过执行计划可以看出无 MR 作业，直接 Fetch Operator")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(": jdbc:hive2://localhost:1000"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("0")]),s._v(">")]),s._v(" explain "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" * from student"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nSTAGE DEPENDENCIES:\n  Stage-0 is a root stage\n\nSTAGE PLANS:\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        TableScan\n          alias: student\n          Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2000")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n          Select Operator\n            expressions: s_id "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", s_name "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", s_birth "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", s_sex "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            outputColumnNames: _col0, _col1, _col2, _col3\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2000")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n            ListSink\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br")])]),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("root@dmcdw-1 /"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# beeline -u jdbc:hive2://localhost:10000 -n hue -p hue")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(": jdbc:hive2://localhost:1000"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("0")]),s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.fetch.task.conversion"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("more"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# more：开启 Fetch 抓取，以下语句不走 MapReduce")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(": jdbc:hive2://localhost:1000"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("0")]),s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_name, s_sex from student limit "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(": jdbc:hive2://localhost:1000"),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("0")]),s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.fetch.task.conversion"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("none"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# none：不开启 Fetch 抓取，以上语句走 MapReduce")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/%E4%B8%8D%E8%B5%B0-MR-%E8%80%97%E6%97%B6-0.1-%E7%A7%92.png",alt:""}}),s._v(" "),a("img",{attrs:{src:"/images/kr/bigdata/hive/%E8%B5%B0-MR-%E8%80%97%E6%97%B6-13.9-%E7%A7%92.png",alt:""}})]),s._v(" "),a("h3",{attrs:{id:"本地模式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#本地模式"}},[s._v("#")]),s._v(" 本地模式")]),s._v(" "),a("p",[s._v("大多数 Hadoop Job 是需要 Hadoop 提供的完整可扩展性来处理大数据集。不过，有时 Hive 输入数据量是非常小，在这种情况下，为查询触发执行任务消耗的时间可能会比实际 Job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务，对于小数据集，执行时间可以明显被缩短。")]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启 local mr，当数据超过最大输入数据量 或者 超过最大输入文件数，不会走 local mr")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.mode.local.auto"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启本地模式，让 Hive 在适当时自动启动这个优化，默认 false")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置本地模式最大输入数据量，当输入数据量小于这个值时采用 local mr 方式，默认 134217728(128 M)")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.mode.local.auto.inputbytes.max"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("51234560")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置本地模式最大输入文件数，当输入文件数小于这个值时采用 local mr 方式，默认 4")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.mode.local.auto.input.files.max"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("当一个 Job 满足如下条件才能真正使用本地模式：")]),s._v(" "),a("ul",[a("li",[s._v("job的输入数据大小必须小于最大输入数据量")]),s._v(" "),a("li",[s._v("job的 map 数必须小于最大输入文件数")]),s._v(" "),a("li",[s._v("job的 reduce 数必须为0或者1")])]),s._v(" "),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/%E5%85%B3%E9%97%AD%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F-%E8%80%97%E6%97%B6-45.5-%E7%A7%92.png",alt:""}}),s._v(" "),a("img",{attrs:{src:"/images/kr/bigdata/hive/%E5%BC%80%E5%90%AF%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F-%E8%80%97%E6%97%B6-1.8-%E7%A7%92.png",alt:""}})]),s._v(" "),a("h3",{attrs:{id:"sql-优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sql-优化"}},[s._v("#")]),s._v(" SQL 优化")]),s._v(" "),a("h4",{attrs:{id:"union-all"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#union-all"}},[s._v("#")]),s._v(" union all")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("--  通过执行计划可以看出该 SQL 对表 student 进行了两次同一字段分组")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("into")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" stu "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("max")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'max'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_sex\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("union")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("all")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("into")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" stu "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("min")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'min'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("--开启动态分区")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dynamic"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dynamic"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("mode")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("nonstrict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- from ... insert into ... ： 使用一张表，可以进行多次插入操作")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("into")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" stu "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("max")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'max'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_sex\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("into")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" stu "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("min")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" s_birth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'min'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("h4",{attrs:{id:"count-distinct"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#count-distinct"}},[s._v("#")]),s._v(" COUNT(DISTINCT)")]),s._v(" "),a("p",[s._v("数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个 Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换，但是需要注意 GROUP BY 造成的数据倾斜问题")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" stu "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 大数据量：先 GROUP BY 再 COUNT 的方式")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("distinct")]),s._v(" s_age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" stu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 小数据量：直接 count(distinct)")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 按年龄去重，由于年龄枚举值是有限的")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 如果转化成 MapReduce，在 Map 阶段，每个 Map 会对 s_age 去重，最终得到 Reduce 数据量也就是 Map 数量 * s_age 枚举值个数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- distinct 命令会在内存中构建一个 hashtable，查找去重的时间复杂度是 O(1)")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- group by 在不同版本间变动比较大，有的版本会用构建 hashtable 形式去重，有的版本会通过排序的方式。排序最优时间复杂度无法到 O(1)")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 另外，第一种方式(group by)去重会转化为两个任务，会消耗更多的磁盘网络 I/O 资源")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("blockquote",[a("p",[s._v("Hive 3.0 中新增了 count(distinct) 优化，通过配置 hive.optimize.countdistinct，即使真的出现数据倾斜也可以自动优化，自动改变 SQL 执行逻辑")])]),s._v(" "),a("h4",{attrs:{id:"group-by"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#group-by"}},[s._v("#")]),s._v(" Group By")]),s._v(" "),a("p",[s._v("默认情况下，Map 阶段同一 Key 数据分发给同一个 Reduce 处理，当一个 Key 数据过大时就发生 Reduce 数据倾斜\n"),a("img",{attrs:{src:"/images/kr/bigdata/hive/Group-By-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.bmp",alt:""}})]),s._v(" "),a("p",[s._v("一种优化方式：并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果")]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.map.aggr "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("                           "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启 Map 端聚合参数设置")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.groupby.mapaggr.checkinterval "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在 Map 端进行聚合操作的条目数目")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.groupby.skewindata "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("                 "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 有数据倾斜时进行负载均衡（默认 false）")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("当 hive.groupby.skewindata = true 时，生成的查询计划会有两个 MR Job：")]),s._v(" "),a("ul",[a("li",[s._v("第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同 Reduce 中，从而达到负载均衡的目的")]),s._v(" "),a("li",[s._v("第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作")])]),s._v(" "),a("h4",{attrs:{id:"小表-join-大表-mapjoin"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#小表-join-大表-mapjoin"}},[s._v("#")]),s._v(" 小表 JOIN 大表(MapJoin)")]),s._v(" "),a("p",[s._v("将 key 相对分散且数据量小的表放在 JOIN 左边，可以使用 MapJoin 让小表先加载进内存，在 Map 端完成 JOIN")]),s._v(" "),a("blockquote",[a("p",[s._v("新版 Hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化，小表放在左边和右边已经没有区别")])]),s._v(" "),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/MapJoin.png",alt:""}})]),s._v(" "),a("p",[s._v("MapJoin：在 Map 阶段将小表读入内存，顺序扫描大表完成 Join，MapJoin 分两个阶段：")]),s._v(" "),a("ol",[a("li",[s._v("通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对 HashTableFiles 进行压缩")]),s._v(" "),a("li",[s._v("MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务")])]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.auto.convert.join "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置自动选择 MapJoin，默认为 true")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.mapjoin.smalltable.filesize "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("25000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 大表小表的阈值设置（默认 25M 以下认为是小表），小表加载进内存")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h4",{attrs:{id:"行列过滤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#行列过滤"}},[s._v("#")]),s._v(" 行列过滤")]),s._v(" "),a("ul",[a("li",[s._v("列处理：在 SELECT 中，只拿需要的列，如果有分区，尽量使用分区过滤，少用 SELECT *")]),s._v(" "),a("li",[s._v("行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤")])]),s._v(" "),a("h3",{attrs:{id:"数据格式优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据格式优化"}},[s._v("#")]),s._v(" 数据格式优化")]),s._v(" "),a("p",[s._v("Hive 提供了多种数据存储组织格式，不同格式对程序的运行效率也会有极大的影响")]),s._v(" "),a("h3",{attrs:{id:"小文件过多优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#小文件过多优化"}},[s._v("#")]),s._v(" 小文件过多优化")]),s._v(" "),a("h4",{attrs:{id:"产生"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#产生"}},[s._v("#")]),s._v(" 产生？")]),s._v(" "),a("ul",[a("li",[s._v("直接向表中 insert 数据：insert into table values ...，每次插入时都会产生一个文件（不管插入数据量有多大）")]),s._v(" "),a("li",[s._v("通过 load 方式加载数据：加载的文件数据源包含大量小文件")]),s._v(" "),a("li",[s._v("通过查询方式加载数据：insert overwrite table ... select ...")]),s._v(" "),a("li",[s._v("动态分区插入数据，产生大量小文件，从而导致 Map 数量剧增")])]),s._v(" "),a("blockquote",[a("p",[s._v("insert 导入数据时会启动 MR，MR 中 Reduce 有多少个就输出多少个文件，则文件数量 = ReduceTask 数量 * 分区数；\n也有很多简单任务没有 Reduce，只有 Map 阶段，则文件数量 = MapTask 数量 * 分区数\n每执行一次 insert 时 Hive 中至少产生一个文件，因为 insert 导入时至少会有一个 MapTask")])]),s._v(" "),a("h4",{attrs:{id:"影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#影响"}},[s._v("#")]),s._v(" 影响？")]),s._v(" "),a("ul",[a("li",[s._v("每个小文件启动一个 Map 任务来完成，而 Map 任务启动和初始化时间远远大于逻辑处理时间，就会造成很大的资源浪费")]),s._v(" "),a("li",[s._v("HDFS 不适合存储大量小文件，过多小文件会占用 NameNode 大量内存，这样 NameNode 内存容量严重制约了集群的扩展")])]),s._v(" "),a("h4",{attrs:{id:"解决"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#解决"}},[s._v("#")]),s._v(" 解决？")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("alter")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" table_name "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("part_field"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("part_value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" concatenate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 使用 hive concatenate 命令，自动合并小文件")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("blockquote",[a("ol",[a("li",[s._v("concatenate 命令只支持 RCFILE 和 ORC 文件类型")]),s._v(" "),a("li",[s._v("concatenate 命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令")]),s._v(" "),a("li",[s._v("当多次 concatenate 后文件数量不变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 有关，可设定每个文件最小大小")])])]),s._v(" "),a("p",[s._v("通过参数调节，设置 Map/Reduce 端的相关参数，以减少 Map/Reduce 数量：")]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Map 输入前进行合并小文件的相关参数：")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.max.split.size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("256000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 每个 Map 最大输入大小(这个值决定了合并后文件的数量)    ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.min.split.size.per.node"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 一个节点上 split 的至少的大小(这个值决定了多个 DataNode 上的文件是否需要合并) ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.min.split.size.per.rack"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 一个交换机下 split 的至少的大小(这个值决定了多个交换机上的文件是否需要合并) ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# CombineHiveInputFormat 底层是 Hadoop CombineFileInputFormat 方法")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 此方法是在 Mapper 中将多个文件合成一个 split 作为输入")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.input.format"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 执行 Map 前进行小文件合并")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Map 输出和 Reduce 输出结束后进行合并的相关参数：  ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapfiles"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Map 端输出进行合并，默认 true  ")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapredfiles"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置 Reduce 端输出进行合并，默认 false")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.size.per.task"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),s._v("*1000*1000"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置合并文件的大小")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.smallfiles.avgsize"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("16000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 当输出文件的平均大小小于该值时，启动一个独立 MapReduce 任务进行文件 merge")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 启用压缩")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.compress.output"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("         "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Hive 的查询结果输出是否进行压缩")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.output.fileoutputformat.compress"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# MapReduce Job 的结果输出是否使用压缩")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br")])]),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Reduce 的个数决定了输出的文件的个数，所以可以调整 Reduce 的个数控制 Hive 表的文件数量")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# hive 中的分区函数 distribute by 正好是控制 MR 中 partition 分区的")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 然后通过设置 reduce 的数量，结合分区函数让数据均衡的进入每个 reduce 即可")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置reduce的数量有两种方式，第一种是直接设置 reduce 个数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.job.reduces"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 第二种是设置每个 reduce 的大小，Hive 会根据数据总大小猜测确定一个 reduce 个数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.reducers.bytes.per.reducer"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5120000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 默认是1G，设置为5G")]),s._v("\n\ndistribute by rand"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 若设置reduce数量为10，则使用 rand()，随机生成一个数 x % 10 ，这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("使用 Hadoop 的 archive 将小文件归档：Hadoop Archive 简称 HAR，是一个高效地将小文件放入 HDFS 块中的文件存档工具，它能够将多个小文件打包成一个 HAR 文件，这样在减少 NameNode 内存使用的同时，仍然允许对文件进行透明的访问。")]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.archive.enabled"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("                  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 用来控制归档是否可用")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.archive.har.parentdir.settable"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 通知 Hive 在创建归档时是否可以设置父目录")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" har.partfile.size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1099511627776")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 控制需要归档文件的大小")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 归档的分区可以查看不能 insert overwrite，必须先 unarchive")]),s._v("\nALTER TABLE A ARCHIVE PARTITION"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2020-12-24'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("hr")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'12'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 使用以下命令进行归档")]),s._v("\nALTER TABLE A UNARCHIVE PARTITION"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2020-12-24'")]),s._v(", "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("hr")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'12'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 对已归档的分区恢复为原文件")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("h3",{attrs:{id:"严格模式-strict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#严格模式-strict"}},[s._v("#")]),s._v(" 严格模式(strict)")]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.mapred.mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("strict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 使用严格模式可以禁止 3 种类型的查询，默认 nonstrict")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.strict.checks.no.partition.filter"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 对于分区表，不加分区字段过滤条件，不能执行")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 对于 order by 语句，必须使用 limit 语句，因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reducer 中进行处理")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.strict.checks.orderby.no.limit"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.strict.checks.cartesian.product"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 限制笛卡尔积的查询（join 时不使用 on，而使用 where）")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("h3",{attrs:{id:"动态分区调整"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#动态分区调整"}},[s._v("#")]),s._v(" 动态分区调整")]),s._v(" "),a("p",[s._v("关系型数据库中，对分区表 Insert 时，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区 (Dynamic Partition)，参数：")]),s._v(" "),a("ul",[a("li",[s._v("hive.exec.dynamic.partition=true：开启动态分区功能，默认开启")]),s._v(" "),a("li",[s._v("hive.exec.dynamic.partition.mode=nonstrict：动态分区模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict：表示允许所有分区字段都可以使用动态分区")]),s._v(" "),a("li",[s._v("hive.exec.max.dynamic.partitions=1000：在所有执行 MR 节点上，最大一共可以创建多少个动态分区")]),s._v(" "),a("li",[s._v("hive.exec.max.dynamic.partitions.pernode=100：在每个执行 MR 节点上，最大可以创建多少个动态分区")]),s._v(" "),a("li",[s._v("hive.exec.max.created.files=100000：整个 MR 中，最大可以创建多少个 HDFS 文件")]),s._v(" "),a("li",[s._v("hive.error.on.empty.partition=false：当有空分区生成时，是否抛出异常")])]),s._v(" "),a("div",{staticClass:"language-xml hdfs-site.xml line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-xml"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("\x3c!-- 控制 DataNode 一次可以打开的文件个数 --\x3e")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("dfs.datanode.max.xcievers"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("8192"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 配置：以第一个表的分区规则，来对应第二个表的分区规则，将第一个表的所有分区，全部拷贝到第二个表中来，")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 第二个表在加载数据的时候，不需要指定分区了，直接用第一个表的分区即可")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("create")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" ori_partitioned"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("bigint")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("time")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("bigint")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" uid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" keyword string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" url_rank "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" click_num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" click_url string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n    PARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("p_time "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("bigint")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("row")]),s._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("fields")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("terminated")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("create")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" ori_partitioned_target"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("bigint")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("time")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("bigint")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" uid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" keyword string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" url_rank "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" click_num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" click_url string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n    PARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("p_time STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("row")]),s._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("fields")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("terminated")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 动态分区：在 SELECT 子句的最后几个字段，必须对应前面 PARTITION 中指定的分区字段，包括顺序")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INSERT")]),s._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" ori_partitioned_target "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PARTITION")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("p_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("time")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" keyword"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" url_rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" click_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" click_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" p_time "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" ori_partitioned"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("h3",{attrs:{id:"并行执行优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#并行执行优化"}},[s._v("#")]),s._v(" 并行执行优化")]),s._v(" "),a("p",[s._v("Hive 会将一个查询转化成一个或多个阶段(Stage)：MapReduce 阶段、抽样阶段、合并阶段、limit 阶段等，默认情况下，Hive 一次只会执行一个阶段。当一个作业的多个阶段并非完全相互依赖时，可以并行执行，缩短执行时间。")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 并行执行在集群资源空闲时能够提高集群资源利用率")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("parallel"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 开启并行执行")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("parallel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("thread"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("number"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 同一个 sql 允许最大并行度，默认 8")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h3",{attrs:{id:"jvm-重用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#jvm-重用"}},[s._v("#")]),s._v(" JVM 重用")]),s._v(" "),a("p",[s._v("JVM 重用是 Hadoop 调优参数的内容，其对 Hive 的性能具有非常大的影响。用于很难避免小文件的场景或者 task 特别多的场景，这类场景大多数执行时间都很短，因为 Hadoop 默认配置通常是使用派生 JVM 来执行 Map/Reduce 任务，在 Hive 执行 MapReduce 任务，JVM 的启动过程会造成很大的开销，尤其是 job 有成千上万个 task 任务时。JVM 重用可以使得 JVM 实例在同一个 job 中重新使用 N 次")]),s._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.job.reuse.jvm.num.tasks"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# N 为重用个数")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-xml mapred-site.xml line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-xml"}},[a("code",[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("mapreduce.job.jvm.numtasks"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("10"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("\x3c!-- 重用个数 --\x3e")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("How many tasks to run per jvm. If set to -1, there is no limit."),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("缺点：开启 JVM 重用将一直占用使用到的 JVM 实例，以便进行重用，直到任务完成后才能释放；若某个作业中有某几个 Reduce task 执行时间要比其他 Reduce task 消耗时间更多时，那么占用的 JVM 实例就会一直空闲着却无法被其他作业使用，直到所有的 task 都结束了才会释放。")]),s._v(" "),a("h3",{attrs:{id:"推测执行优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#推测执行优化"}},[s._v("#")]),s._v(" 推测执行优化")]),s._v(" "),a("p",[s._v("在分布式集群环境下，因负载不均衡或资源分布不均等原因，会造成同一个作业多个任务之间运行速度不一致，有些任务运行速度可能明显慢于其他任务，则这些任务会拖慢作业的整体执行进度。\n为了避免这种情况发生，Hadoop 采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。")]),s._v(" "),a("div",{staticClass:"language-xml mapred-site.xml line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-xml"}},[a("code",[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("mapreduce.map.speculative"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("true"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("If true, then multiple instances of some map tasks may be executed in parallel."),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("mapreduce.reduce.speculative"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("true"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("If true, then multiple instances of some reduce tasks may be executed in parallel."),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("description")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tasks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("speculative"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("execution"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("缺点：若因输入数据量很大而需要执行长时间的 Map/Reduce task 时，那么启动推测执行会造成资源浪费和压力是非常巨大的")])])}),[],!1,null,null,null);t.default=e.exports}}]);