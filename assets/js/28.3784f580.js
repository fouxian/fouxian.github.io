(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{483:function(s,t,a){"use strict";a.r(t);var e=a(34),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"hive-工作原理-hive-sql-编译过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-工作原理-hive-sql-编译过程"}},[s._v("#")]),s._v(" Hive 工作原理：Hive SQL 编译过程")]),s._v(" "),a("p",[s._v("本文引自："),a("a",{attrs:{href:"https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("Hive SQL 的编译过程"),a("OutboundLink")],1),s._v("\n参考："),a("a",{attrs:{href:"https://cloud.tencent.com/developer/article/1843197",target:"_blank",rel:"noopener noreferrer"}},[s._v("Hive SQL底层执行过程详细剖析"),a("OutboundLink")],1)]),s._v(" "),a("h2",{attrs:{id:"hql-编译过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hql-编译过程"}},[s._v("#")]),s._v(" HQL 编译过程")]),s._v(" "),a("ul",[a("li",[s._v("词法/语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象语法树 AST Tree")]),s._v(" "),a("li",[s._v("语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock")]),s._v(" "),a("li",[s._v("生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree")]),s._v(" "),a("li",[s._v("优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并 Operator，达到减少 MapReduce Job，减少数据传输及 shuffle 数据量")]),s._v(" "),a("li",[s._v("生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务")]),s._v(" "),a("li",[s._v("优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划")])]),s._v(" "),a("h3",{attrs:{id:"词法-语法解析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#词法-语法解析"}},[s._v("#")]),s._v(" 词法/语法解析")]),s._v(" "),a("blockquote",[a("p",[s._v("Hive 使用 Antlr 实现 SQL 的词法和语法解析，Antlr 是一种语言识别工具，可以用来构造领域语言。使用 Antlr 构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr 完成了词法分析、语法分析、语义分析、中间代码生成的过程。")])]),s._v(" "),a("p",[s._v("根据 Antlr 定义的 SQL 语法规则，将相关 SQL 进行词法、语法解析，转化为抽象语法树 Abstract Syntax Tree，AST Tree")]),s._v(" "),a("h3",{attrs:{id:"语义解析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#语义解析"}},[s._v("#")]),s._v(" 语义解析")]),s._v(" "),a("p",[s._v("AST Tree 生成后需要进一步抽象和结构化，形成 QueryBlock：QueryBlock 是一条 SQL 最基本的组成单元，包括三个部分：输入源，计算过程，输出；简单来讲 QueryBlock 就是一个子查询；QueryBlock 的生成过程为一个递归过程，先序遍历 AST Tree ，遇到不同的 Token 节点(理解为特殊标记)，保存到相应的属性中。")]),s._v(" "),a("h3",{attrs:{id:"生成逻辑执行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#生成逻辑执行计划"}},[s._v("#")]),s._v(" 生成逻辑执行计划")]),s._v(" "),a("p",[s._v("遍历 QueryBlock，翻译为逻辑执行操作树 OperatorTree，Hive 生成的 MapReduce 任务的 Map/Reduce 阶段均由 OperatorTree 组成，其中，逻辑操作符(Operator)就是在 Map/Reduce 阶段完成单一特定的操作。基本的操作符包括：")]),s._v(" "),a("ul",[a("li",[s._v("TableScanOperator：从 Map 接口原始输入表的数据，控制扫描表的数据行数，标记是从原表中取数据")]),s._v(" "),a("li",[s._v("SelectOperator：选取操作")]),s._v(" "),a("li",[s._v("FilterOperator：过滤操作")]),s._v(" "),a("li",[s._v("JoinOperator：Join 操作")]),s._v(" "),a("li",[s._v("GroupByOperator：分组聚合操作")]),s._v(" "),a("li",[s._v("ReduceSinkOperator：Map 端字段组合序列化为 Reduce Key/value, Partition Key，只可能出现在 Map 阶段，同时也标志着 Map 阶段结束")])]),s._v(" "),a("blockquote",[a("p",[s._v("Operator 在 Map Reduce 阶段之间的数据传递都是一个流式过程，每一个 Operator 对一行数据完成操作之后将数据传递给 childOperator 计算\n由于 Join/GroupBy/OrderBy 均需要在 Reduce 阶段完成，所以在生成相应操作的 Operator 之前都会先生成一个 ReduceSinkOperator，将字段组合并序列化为 Reduce Key/value, Partition Key")])]),s._v(" "),a("h3",{attrs:{id:"优化逻辑执行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#优化逻辑执行计划"}},[s._v("#")]),s._v(" 优化逻辑执行计划")]),s._v(" "),a("p",[s._v("大部分逻辑层优化通过变换 OperatorTree，合并操作符，达到减少 MapReduce Job，减少 Shuffle 数据量的目的，Hive 中的逻辑查询优化可以大致分为以下几类：")]),s._v(" "),a("ul",[a("li",[s._v("投影修剪")]),s._v(" "),a("li",[s._v("推导传递谓词")]),s._v(" "),a("li",[s._v("谓词下推")]),s._v(" "),a("li",[s._v("将 Select-Select，Filter-Filter 合并为单个操作")]),s._v(" "),a("li",[s._v("多路 Join")]),s._v(" "),a("li",[s._v("查询重写以适应某些列值的 Join 倾斜")])]),s._v(" "),a("h3",{attrs:{id:"生成物理执行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#生成物理执行计划"}},[s._v("#")]),s._v(" 生成物理执行计划")]),s._v(" "),a("p",[s._v("生成物理执行计划即是将逻辑执行计划生成的 OperatorTree 转化为 MapReduce Job 的过程，主要分为几个阶段：")]),s._v(" "),a("ol",[a("li",[s._v("对输出表生成 MoveTask")]),s._v(" "),a("li",[s._v("从 OperatorTree 的其中一个根节点向下深度优先遍历")]),s._v(" "),a("li",[s._v("ReduceSinkOperator 标示 Map/Reduce 的界限，多个 Job 间的界限")]),s._v(" "),a("li",[s._v("遍历其他根节点，遇过碰到 JoinOperator 合并 MapReduceTask")]),s._v(" "),a("li",[s._v("生成 StatTask 更新元数据")]),s._v(" "),a("li",[s._v("剪断 Map 与 Reduce 间的 Operator 关系")])]),s._v(" "),a("h3",{attrs:{id:"优化物理执行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#优化物理执行计划"}},[s._v("#")]),s._v(" 优化物理执行计划")]),s._v(" "),a("p",[s._v("Hive 中的物理优化可以大致分为以下几类：")]),s._v(" "),a("ul",[a("li",[s._v("分区修剪(Partition Pruning)")]),s._v(" "),a("li",[s._v("基于分区和桶的扫描修剪(Scan pruning)")]),s._v(" "),a("li",[s._v("如果查询基于抽样，则扫描修剪")]),s._v(" "),a("li",[s._v("在某些情况下，在 map 端应用 Group By")]),s._v(" "),a("li",[s._v("在 mapper 上执行 Join")]),s._v(" "),a("li",[s._v("优化 Union，使 Union 只在 map 端执行")]),s._v(" "),a("li",[s._v("在多路 Join 中，根据用户提示决定最后流哪个表")]),s._v(" "),a("li",[s._v("删除不必要的 ReduceSinkOperators")]),s._v(" "),a("li",[s._v("对于带有 Limit 子句的查询，减少需要为该表扫描的文件数")]),s._v(" "),a("li",[s._v("对于带有 Limit 子句的查询，通过限制 ReduceSinkOperator 生成的内容来限制来自 mapper 的输出")]),s._v(" "),a("li",[s._v("减少用户提交的 SQL 查询所需的 Tez 作业数量")]),s._v(" "),a("li",[s._v("如果是简单的提取查询，避免使用 MapReduce 作业")]),s._v(" "),a("li",[s._v("对于带有聚合的简单获取查询，执行不带 MapReduce 任务的聚合")]),s._v(" "),a("li",[s._v("重写 Group By 查询使用索引表代替原来的表")]),s._v(" "),a("li",[s._v("当表扫描之上的谓词是相等谓词且谓词中的列具有索引时，使用索引扫描")])]),s._v(" "),a("h2",{attrs:{id:"逻辑执行计划示例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#逻辑执行计划示例"}},[s._v("#")]),s._v(" 逻辑执行计划示例")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("explain")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" score cluster "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# HQL 语句被转换为由多个 stage(阶段) 组成的序列（有向无环图 DAG），")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 这些 stage 包括：MR stage、元数据存储的 stage、文件系统操作的 stage")]),s._v("\nSTAGE DEPENDENCIES:                     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 各个 stage 间的依赖关系")]),s._v("\n  Stage-1 is a root stage               "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# root stage，执行的起始 stage")]),s._v("\n  Stage-0 depends on stages: Stage-1    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 依赖于 stage-1")]),s._v("\n\nSTAGE PLANS:                            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 各个 stage 的执行计划")]),s._v("\n  Stage: Stage-1                        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# stage-1 的执行计划")]),s._v("\n    Map Reduce                          "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# MapReduce 作业的执行计划")]),s._v("\n      Map Operator Tree:                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map 端的执行计划树")]),s._v("\n          TableScan                     "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表扫描操作")]),s._v("\n            alias: score                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表名称/别名")]),s._v("\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表统计信息")]),s._v("\n            Select Operator             "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 选取操作")]),s._v("\n              expressions: s_id "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", c_id "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", s_score "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 选取需要查询的表字段名/类型")]),s._v("\n              outputColumnNames: _col0, _col1, _col2        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 输出的列名")]),s._v("\n              Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 表统计信息")]),s._v("\n              Reduce Output Operator    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 输出到 Reduce 的操作")]),s._v("\n                key expressions: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map key")]),s._v("\n                "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("sort")]),s._v(" order: +           "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 排序")]),s._v("\n                Map-reduce partition columns: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n                Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n                value expressions: _col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", _col2 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Map value")]),s._v("\n      Execution mode: vectorized\n      Reduce Operator Tree:             "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Reduce 端的执行计划树")]),s._v("\n        Select Operator                 "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 选取操作")]),s._v("\n          expressions: KEY.reducesinkkey0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", VALUE._col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(", VALUE._col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("type: int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n          outputColumnNames: _col0, _col1, _col2\n          Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n          File Output Operator          "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 文件输出操作")]),s._v("\n            compressed: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("false")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 是否压缩")]),s._v("\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1620")]),s._v(" Basic stats: COMPLETE Column stats: NONE\n            table:\n                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\n  Stage: Stage-0\n    Fetch Operator                      "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 客户端取数据操作")]),s._v("\n      limit: -1\n      Processor Tree:\n        ListSink\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br")])]),a("blockquote",[a("p",[s._v("application -> job -> stage -> task")])]),s._v(" "),a("h2",{attrs:{id:"sql-编译成-mapreduce-的原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sql-编译成-mapreduce-的原理"}},[s._v("#")]),s._v(" SQL 编译成 MapReduce 的原理")]),s._v(" "),a("h3",{attrs:{id:"join-实现原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#join-实现原理"}},[s._v("#")]),s._v(" Join 实现原理")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" o"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("orderid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" o "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("user")]),s._v(" u "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" o"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("uid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("在 Map 输出 value 中为不同表数据打上 tag 标记，在 Reduce 阶段根据 tag 判断数据来源，MapReduce 过程如下：\n"),a("img",{attrs:{src:"/images/kr/bigdata/hive/MapReduce-CommonJoin.png",alt:""}})]),s._v(" "),a("h3",{attrs:{id:"group-by-实现原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#group-by-实现原理"}},[s._v("#")]),s._v(" Group By 实现原理")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" isonline"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" city "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" isonline"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("将 Group By 字段组合为 Map 的输出 key 值，利用 MapReduce 的排序，在 Reduce 阶段保存 LastKey 区分不同的 key，MapReduce过程如下：\n"),a("img",{attrs:{src:"/images/kr/bigdata/hive/MapReduce-GroupBy.png",alt:""}})]),s._v(" "),a("h3",{attrs:{id:"distinct-实现原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#distinct-实现原理"}},[s._v("#")]),s._v(" Distinct 实现原理")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" dealid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("distinct")]),s._v(" uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" dealid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("当只有一个 Distinct 字段时，如果不考虑 Map 阶段的 Hash Group By，只需要将 Group By 字段和 Distinct 字段组合为 Map 输出 key，利用 MapReduce 的排序，同时将 Group By 字段作为 Reduce 的 key，在 Reduce 阶段保存 LastKey 即可完成去重：\n"),a("img",{attrs:{src:"/images/kr/bigdata/hive/MapReduce-Distinct.png",alt:""}})]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" dealid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("distinct")]),s._v(" uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("distinct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("date")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" dealid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("如果有多个 DIstinct 字段，实现方式有两种：")]),s._v(" "),a("ol",[a("li",[s._v("如果仍然按照上面一个 Distinct 字段的方法，则无法跟据 uid 和 date 分别排序，也就无法通过 LastKey 去重，仍然需要在 Reduce 阶段在内存中通过 Hash 去重")])]),s._v(" "),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/MapReduce-Multi-Distinct-1.png",alt:""}})]),s._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[s._v("先对所有的 Distinct 字段编号，每行数据生成 n 行数据，那么相同字段就会分别排序，这时只需要在 Reduce 阶段记录 LastKey 即可去重；这种实现方式很好的利用了 MapReduce 的排序，节省了 Reduce 阶段去重的内存消耗，但是缺点是增加了 Shuffle 的数据量")])]),s._v(" "),a("p",[a("img",{attrs:{src:"/images/kr/bigdata/hive/MapReduce-Multi-Distinct-2.png",alt:""}})]),s._v(" "),a("blockquote",[a("p",[s._v("注：在生成 Reduce value 时，除第一个 Distinct 字段所在行需保留 value 值，其余 Distinct 数据行 value 字段均可为空")])])])}),[],!1,null,null,null);t.default=n.exports}}]);