(window.webpackJsonp=window.webpackJsonp||[]).push([[34],{490:function(a,t,e){"use strict";e.r(t);var r=e(34),s=Object(r.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"spark-运行架构及作业提交流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark-运行架构及作业提交流程"}},[a._v("#")]),a._v(" Spark 运行架构及作业提交流程")]),a._v(" "),e("h2",{attrs:{id:"运行架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#运行架构"}},[a._v("#")]),a._v(" 运行架构")]),a._v(" "),e("p",[a._v("独立部署模式下，一个基于 Master-Slave 标准的 Spatk 执行时的基本结构，其中 Driver(Master) 负责管理整个集群中的作业任务调度，Executor(Slave) 负责实际执行任务：\n"),e("img",{attrs:{src:"/images/kr/bigdata/spark/Spark-%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84.png",alt:""}})]),a._v(" "),e("p",[a._v("执行过程：")]),a._v(" "),e("ul",[e("li",[a._v("Driver 创建 SparkContext 后连接到集群资源管理器，资源管理器为用户程序分配计算资源，并启动 Executor")]),a._v(" "),e("li",[a._v("Dirver 将计算程序划分为不同的作业执行阶段和多个任务 Task，之后将 Task 发送给 Executor")]),a._v(" "),e("li",[a._v("Executor 负责执行 Task，并将执行状态汇报给 Driver，同时将当前节点资源的使用情况汇报给集群资源管理器")])]),a._v(" "),e("p",[a._v("核心组件：")]),a._v(" "),e("ul",[e("li",[a._v("Driver：驱动器节点，用于执行作业中 main 方法，负责实际代码执行工作，管理整个集群中的作业任务调度及跟踪 Executor 的执行情况")]),a._v(" "),e("li",[a._v("Executor：Executor 是一个 JVM 进程，负责运行具体任务，任务彼此之间相互独立；核心功能：\n"),e("ul",[e("li",[a._v("负责运行组成 Spark 应用的任务，并将结果返回给 Driver 进程")]),a._v(" "),e("li",[a._v("通过自身的块管理器（Block Manager）为 RDD 提供内存式存储；RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算")])])])]),a._v(" "),e("p",[a._v("在提交应用时，可以提供参数指定计算节点的个数，以及对应的资源（Executor 内存大小和使用的虚拟 CPU 核（Core）数量）")]),a._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"center"}},[a._v("参数")]),a._v(" "),e("th",{staticStyle:{"text-align":"center"}},[a._v("备注")])])]),a._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"center"}},[a._v("--num-executors")]),a._v(" "),e("td",{staticStyle:{"text-align":"center"}},[a._v("配置 Executor 的数量")])]),a._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[a._v("--executor-memory")]),a._v(" "),e("td",{staticStyle:{"text-align":"center"}},[a._v("配置每个 Executor 的内存大小")])]),a._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[a._v("--executor-cores")]),a._v(" "),e("td",{staticStyle:{"text-align":"center"}},[a._v("配置每个 Executor 的虚拟 CPU core 数量")])])])]),a._v(" "),e("h2",{attrs:{id:"运行流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#运行流程"}},[a._v("#")]),a._v(" 运行流程")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/Spark-%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.jpg",alt:""}})]),a._v(" "),e("ul",[e("li",[a._v("任务提交后，都会先启动 Driver 程序")]),a._v(" "),e("li",[a._v("随后 Driver 向集群管理器注册应用程序")]),a._v(" "),e("li",[a._v("集群管理器根据此任务的配置文件分配 Executor 并启动")]),a._v(" "),e("li",[a._v("Driver 开始执行 main 函数，Spark 查询为懒执行，当执行 Action 算子时开始反向推算，根据宽依赖进行 Stage 划分，随后每一个 Stage 对应一个 Taskset，其中有多个 Task，查找可用资源 Executor 进行调度 进行调度")]),a._v(" "),e("li",[a._v("根据本地化原则，Task 会被分发到指定的 Executor 执行，在任务的过程中 Executor 也会不断与 Driver 进行通信，报告任务运情况")])]),a._v(" "),e("h3",{attrs:{id:"spark-on-yarn"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark-on-yarn"}},[a._v("#")]),a._v(" Spark On Yarn")]),a._v(" "),e("p",[a._v("Spark 应用程序提交到 Yarn 中执行时，两种部署：Client/Cluster；主要区别在于：Driver 程序的运行节点位置")]),a._v(" "),e("h4",{attrs:{id:"yarn-client"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#yarn-client"}},[a._v("#")]),a._v(" Yarn Client")]),a._v(" "),e("p",[a._v("Yarn Client 一般用于测试，其 Driver 运行在客户端\n"),e("img",{attrs:{src:"/images/kr/bigdata/spark/Spark-On-Yarn-Client.jpg",alt:""}})]),a._v(" "),e("ul",[e("li",[a._v("执行脚本提交任务，实际是启动一个 SparkSubmit JVM 进程，并与 ResourceManager 通讯申请启动 ApplicationMaster")]),a._v(" "),e("li",[a._v("ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存")]),a._v(" "),e("li",[a._v("ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程")]),a._v(" "),e("li",[a._v("Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 main 函数")]),a._v(" "),e("li",[a._v("执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 Stage，每个 Stage 生成对应的 TaskSet，将 task 分发到各个 Executor 上执行")])]),a._v(" "),e("h4",{attrs:{id:"yarn-cluster"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#yarn-cluster"}},[a._v("#")]),a._v(" Yarn Cluster")]),a._v(" "),e("p",[a._v("Yarn Cluster 一般用于生产环境，其 Driver 运行在 Yarn\n"),e("img",{attrs:{src:"/images/kr/bigdata/spark/Spark-On-Yarn-Cluster.jpg",alt:""}})]),a._v(" "),e("ul",[e("li",[a._v("执行脚本提交任务，实际是启动一个 SparkSubmit JVM 进程，并与 ResourceManager 通讯申请启动 ApplicationMaster")]),a._v(" "),e("li",[a._v("RM 分配 Container，在 NM上启动 AM，AM 启动 Driver 线程，执行作业")]),a._v(" "),e("li",[a._v("Driver 启动后向 RM 申请 Executor 内存，RM 接到 AM 的资源申请后会分配 Container，然后在 NM 上启动 Executor 进程")]),a._v(" "),e("li",[a._v("Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 main 函数")]),a._v(" "),e("li",[a._v("执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 Stage，每个 Stage 生成对应的 TaskSet，将 task 分发到各个 Executor 上执行")])]),a._v(" "),e("h3",{attrs:{id:"standalone"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#standalone"}},[a._v("#")]),a._v(" Standalone")]),a._v(" "),e("ul",[e("li",[a._v("Master(RM)：进程，负责资源调度分配及集群的监控")]),a._v(" "),e("li",[a._v("Worker(NM)：进程，负责 RDD 的分区缓存，及对 RDD 上的分区计算")])]),a._v(" "),e("h4",{attrs:{id:"standalone-cluster"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#standalone-cluster"}},[a._v("#")]),a._v(" Standalone Cluster")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/Standalone-Cluster.jpg",alt:""}})]),a._v(" "),e("p",[a._v("任务提交后，Master 会找到一个 Worker 启动 Driver。 Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 Stage，每个 Stage 生成对应的 taskSet，之后将 Task 分发到各个 Executor 执行")]),a._v(" "),e("h4",{attrs:{id:"standalone-client"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#standalone-client"}},[a._v("#")]),a._v(" Standalone Client")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/Standalone-Client.jpg",alt:""}})]),a._v(" "),e("p",[a._v("Driver 在任务提交的本地机器上运行。Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 Stage，每个 Stage 生成对应的 TaskSet，之后将 Task 分发到各个 Executor 上执行")]),a._v(" "),e("h2",{attrs:{id:"任务调度机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#任务调度机制"}},[a._v("#")]),a._v(" 任务调度机制")]),a._v(" "),e("h3",{attrs:{id:"总体调度流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#总体调度流程"}},[a._v("#")]),a._v(" 总体调度流程")]),a._v(" "),e("p",[a._v("一个 Spark 应用程序包括 Job、Stage、Task 三个概念：")]),a._v(" "),e("ul",[e("li",[a._v("Job：是以 Action 方法为界，遇到一个 Action 方法则触发一个 Job")]),a._v(" "),e("li",[a._v("Stage：是 Job 的子集，以 RDD 宽依赖(即 Shuffle)为界，遇到 Shuffle 做一次划分")]),a._v(" "),e("li",[a._v("Task：是 Stage 的子集，以并行度(分区数)来衡量，分区数是多少则有多少个 task")])]),a._v(" "),e("p",[a._v("Spark 任务调度总体分两路：Stage 级调度和 Task 级调度，总体调度流程：\n"),e("img",{attrs:{src:"/images/kr/bigdata/spark/Spark-%E6%80%BB%E4%BD%93%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B.jpg",alt:""}})]),a._v(" "),e("p",[a._v("Spark RDD 通过其 Transactions 算子，形成了 RDD 血缘(依赖)关系图，即 DAG，最后通过 Action 算子，触发 Job 并调度执行，执行过程中会创建两个调度器：DAGScheduler 和 TaskScheduler")]),a._v(" "),e("ul",[e("li",[a._v("DAGScheduler：负责 Stage 级调度，主要是将 job 切分成若干 Stages，并将每个 Stage 打包成 TaskSet 交给 TaskScheduler 调度")]),a._v(" "),e("li",[a._v("TaskScheduler：负责 Task 级调度，将 TaskSet 按照指定的调度策略分发到 Executor 上执行，调度过程中 SchedulerBackend 负责申请可用资源，其中 SchedulerBackend 有多种实现，分别对接不同的资源管理系统")])]),a._v(" "),e("h3",{attrs:{id:"stage-级调度"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#stage-级调度"}},[a._v("#")]),a._v(" Stage 级调度")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/Stage-%E7%BA%A7%E8%B0%83%E5%BA%A6%E6%96%B9%E6%B3%95%E6%B5%81%E7%A8%8B.jpg",alt:""}})]),a._v(" "),e("ul",[e("li",[a._v("Action 算子触发一个 Job，Job 由最终的 RDD 和 Action 算子封装而成")]),a._v(" "),e("li",[a._v("SparkContext 将 Job 交给 DAGScheduler 提交，它会根据 RDD 的血缘关系 DAG 进行切分，将一个 Job 划分为若干 Stages，划分的 Stages 分两类：\n"),e("ul",[e("li",[a._v("ResultStage：为 DAG 最下游的 Stage，基本上由 Action 算子决定，即将一个函数应用在 RDD 各个分区的数据集上，意味着 Job 运行结束")]),a._v(" "),e("li",[a._v("ShuffleMapStage：为下游 Stage 准备数据，其结束伴随着 shuffle 文件的写磁盘")])])]),a._v(" "),e("li",[a._v("具体划分策略：由最终的 RDD 不断通过依赖回溯判断父依赖是否宽依赖，即以 Shuffle 为界，划分 Stage，窄依赖的 RDD 被划分到同一个 Stage 中，可进行 pipeline 式计算")]),a._v(" "),e("li",[a._v("Stage 提交：只有在父 Stage 执行完毕后才提交当前 Stage：一个分区对应一个 Task，将 Task 信息序列化并打包成 TaskSet 交给 TaskScheduler，并监控 Stage 运行状态")]),a._v(" "),e("li",[a._v("失败重试：只有 Executor 丢失或 Task 由于 Fetch 失败才需要重新提交 Stage 以调度运行失败的任务，其他类型的 Task 失败会在 TaskScheduler 的调度过程中重试")])]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/WordCount-Stage.jpg",alt:""}})]),a._v(" "),e("ul",[e("li",[a._v("Job：由 saveAsTextFile Action 算子触发，组成：Job = [RDD-3 + saveAsTextFile]")]),a._v(" "),e("li",[a._v("依赖回溯：从 RDD-3 回溯至 RDD-0；宽依赖：RDD-3 依赖 RDD-2；窄依赖：RDD-2 依赖 RDD-1 依赖 RDD-0")]),a._v(" "),e("li",[a._v("Stage 划分：ResultStage：RDD-3；ShuffleMapStage：RDD-2、RDD-1、RDD-0，形成 pipeline 操作")]),a._v(" "),e("li",[a._v("ShuffleMapStage pipeline 操作：数据记录会直接执行从 RDD-0 到 RDD-2 的转化，本质是深度优先搜索算法")])]),a._v(" "),e("h3",{attrs:{id:"task-级调度"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#task-级调度"}},[a._v("#")]),a._v(" Task 级调度")]),a._v(" "),e("p",[a._v("TaskSet 会进一步封装成 TaskSetManager 加入调度队列中，TaskSetManager 负责监控管理同一个 Stage 中的 Tasks，TaskScheduler 就是以 TaskSetManager 为单位调度任务；TaskScheduler 会从调度队列中按照指定调度策略选择 TaskSetManager 去调度运行：\n"),e("img",{attrs:{src:"/images/kr/bigdata/spark/Task-%E7%BA%A7%E8%B0%83%E5%BA%A6%E6%96%B9%E6%B3%95%E6%B5%81%E7%A8%8B.jpg",alt:""}})]),a._v(" "),e("p",[a._v("TaskScheduler 支持的调度策略：FIFO，默认；FAIR\n由于一个 partition 对应一个 Task，此 partition 的优先位置就是 Task 的优先位置，对于要提交到 TaskScheduler 的 TaskSet 中的每一个 Task，该 Task 优先位置与其对应的 partition 对应的优先位置一致。根据每个 Task 的优先位置，确定 Task 的 Locality 级别，Locality 一共有五种，由高到低顺序：")]),a._v(" "),e("ul",[e("li",[a._v("PROCESS_LOCAL：进程本地化，Task 和数据在同一个 Executor 中")]),a._v(" "),e("li",[a._v("NODE_LOCAL：节点本地化，Task 和数据在同一个节点中，但是 Task 和数据不在同一个 Executor 中，数据需要在进程间行传输")]),a._v(" "),e("li",[a._v("RACK_LOCAL：机架本地化，Task 和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输")]),a._v(" "),e("li",[a._v("NO_PREF：对于 Task 来说，从哪里获取都一样没有好坏之分")]),a._v(" "),e("li",[a._v("ANY：Task 和数据可以在集群的任何地方，而且不在一个机架中，性能最差")])]),a._v(" "),e("p",[a._v("本地化调度：在调度执行时，Spark 调度总是会尽量让每个 Task 以最高的本地性级别启动，当一个 Task 以 X 本地性级别启动，但是该本地性级别对应的所有节点都没空闲资源而启动失败，此时并不会马上降低本地性级别启动而是在某个时间长度内再次以 X 本地性级别启动该 Task，若超过限时时间则降级启动，去尝试下一个本地性级别，依次类推；可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的 Executor 可能就会有相应的资源去执行此 Task，这就在一定程度上提高了运行性能。\n失败重试：TaskSetManager 会记录 Task 的次数，如果还没有超过最大失败次数，那么就把它放回待调度的 Task 池子中，否则整个 Application 失败\n黑名单机制：在记录 Task 失败次数过程中，会记录它上一次失败所在的 Executor Id 和 Host，及对应的拉黑时间，下次调度时避开该节点")]),a._v(" "),e("h2",{attrs:{id:"spark-shuffle"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark-shuffle"}},[a._v("#")]),a._v(" Spark Shuffle")]),a._v(" "),e("h3",{attrs:{id:"hashshuffle"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hashshuffle"}},[a._v("#")]),a._v(" HashShuffle")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/%E6%9C%AA%E4%BC%98%E5%8C%96-HashShuffle.png",alt:""}})]),a._v(" "),e("p",[a._v("每个 MapTask 通过 Hash 算法输出三类本地小文件，总共输出 12 份本地小文件，再聚合到 3 个处理不同数据类别的 ReduceTask")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/%E4%BC%98%E5%8C%96%E5%90%8E-HashShuffle.png",alt:""}})]),a._v(" "),e("p",[a._v("优化：开启合并机制，复用 buffer，同样 Key 放在同一个 Buffer 里，然后把 Buffer 中的数据写入以 Core 数量为单位的本地文件中（一个 Core 只有一种类型 Key 的数据），每 1 个 Task 所在的进程中，分别写入共同进程中的 3 份本地文件，共 6 个本地小文件")]),a._v(" "),e("div",{staticClass:"language-bash line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[a._v("spark.shuffle.consolidateFiles "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("true")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("h3",{attrs:{id:"sortshuffle"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sortshuffle"}},[a._v("#")]),a._v(" SortShuffle")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/%E6%99%AE%E9%80%9A-SortShuffle.jpg",alt:""}})]),a._v(" "),e("p",[a._v("数据通过 reduceByKey 写入 Map，通过 Map 局部聚合再写入内存，并判断内存是否达到阈值，若达到就会将内存数据写入磁盘。\n在溢写磁盘前，先根据 key 进行排序，再以每批一万条分批写入到磁盘文件中，通过缓冲区溢写的方式，每次溢写都会产生一个临时磁盘文件，最后再合并到一个文件中，同时单独写一份索引文件，标识下游各个 Task 的数据在文件中索引：start offset/end offset。")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/bypass-SortShuffle.jpg",alt:""}})]),a._v(" "),e("p",[a._v("bypass 运行机制触发条件：（与普通 SortShuffle 相比，无需排序）")]),a._v(" "),e("ul",[e("li",[a._v("shuffle reduce task 数量小于等于 spark.shuffle.sort.bypassMergeThreshold 参数值，默认 200")]),a._v(" "),e("li",[a._v("不是聚合类的 shuffle 算子（比如 reduceByKey）")])]),a._v(" "),e("p",[a._v("此时 task 会为每个 reduce 端的 task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 至对应的磁盘文件中。先写入内存缓冲，缓冲写满后再溢写到磁盘文件，最后合并成一个磁盘文件，并创建单独的索引文件。")]),a._v(" "),e("h2",{attrs:{id:"并行度-parallelism"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#并行度-parallelism"}},[a._v("#")]),a._v(" 并行度 Parallelism")]),a._v(" "),e("p",[a._v("在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，将整个集群并行执行任务的数量称之为并行度。\n那么一个作业的并行度取决于框架的默认配置，应用程序也可以在运行过程中动态修改。")]),a._v(" "),e("h2",{attrs:{id:"有向无环图-dag"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#有向无环图-dag"}},[a._v("#")]),a._v(" 有向无环图 DAG")]),a._v(" "),e("p",[a._v("第一代计算引擎 Hadoop MapReduce 将计算分为 Map/Reduce 两个阶段，这样有个弊端：对于上层应用来说，就不得去拆分算法，甚至在上层应用中实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。\n由于这样的弊端，催生了支持 DAG 框架的产生，支持 DAG 的框架被划分为第二代计算引擎，如 Tez 以及更上层的 Oozie。\n以 Spark 为代表的第三代的计算引擎特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。\n由 Spark 程序直接映射成的数据流的高级抽象模型，简单理解就是将整个程序计算的执行过程用图形表示出来，这样更直观，更便于理解，可以用于表示程序的拓扑结构：")]),a._v(" "),e("p",[e("img",{attrs:{src:"/images/kr/bigdata/spark/Spark-DAG.jpg",alt:""}})]),a._v(" "),e("p",[a._v("DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。")])])}),[],!1,null,null,null);t.default=s.exports}}]);